{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Soal 1\n"
      ],
      "metadata": {
        "id": "ZBuVhPo1u2t3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library dan Dataset yang Diperlukan"
      ],
      "metadata": {
        "id": "pF9_ASerL_CI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "DIuPmh5yrwu0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "test_bank = pd.read_csv(\"test_bank.csv\")\n",
        "train_bank = pd.read_csv(\"train_bank.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tahap 1 : Eksplorasi dan Pembersihan Data"
      ],
      "metadata": {
        "id": "Vjg4BUwFu70b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fungsi preproses data\n",
        "def preprocess (data):\n",
        "  label_encoder = LabelEncoder()\n",
        "  for column in data:\n",
        "      if data[column].dtype in ['object']:\n",
        "       data[column] = label_encoder.fit_transform(data[column])\n",
        "       print(f\"Mapping untuk kolom  non numerik{column}: {dict(enumerate(label_encoder.classes_))}\")\n",
        "\n",
        "      # non numerics have label now\n",
        "       invalid_values = data[data[column] < 0]\n",
        "       print(f\"Distribusi kolom numerik {column}:\")\n",
        "       print(data[column].value_counts(), \"\\n\")\n",
        "       print(f\"Nilai minimum untuk kolom numerik {column}:\")\n",
        "       print(data[column].min(skipna=True), \"\\n\")\n",
        "       print(f\"Nilai maksimum untuk kolom numerik {column}:\")\n",
        "       print(data[column].max(skipna=True),\"\\n\")\n",
        "\n",
        "       if not invalid_values.empty:\n",
        "           print(f\"Nilai tidak valid pada kolom {column}:\")\n",
        "           print(invalid_values, \"\\n\")\n"
      ],
      "metadata": {
        "id": "Q8oug0i-6ODz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_bank.describe()\n",
        "test_bank.info()\n",
        "train_bank.describe()\n",
        "train_bank.info()\n",
        "\n",
        "preprocess(train_bank)\n",
        "preprocess(test_bank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaKlA4PmvBS-",
        "outputId": "fafc097d-0aaf-4e6c-ca47-193cb2eb445c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 33034 entries, 0 to 33033\n",
            "Data columns (total 14 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   id               33034 non-null  int64  \n",
            " 1   CustomerId       33034 non-null  int64  \n",
            " 2   Surname          33034 non-null  object \n",
            " 3   CreditScore      33034 non-null  int64  \n",
            " 4   Geography        33034 non-null  object \n",
            " 5   Gender           33034 non-null  object \n",
            " 6   Age              33034 non-null  float64\n",
            " 7   Tenure           33034 non-null  int64  \n",
            " 8   Balance          33034 non-null  float64\n",
            " 9   NumOfProducts    33034 non-null  int64  \n",
            " 10  HasCrCard        33034 non-null  float64\n",
            " 11  IsActiveMember   33034 non-null  float64\n",
            " 12  EstimatedSalary  33034 non-null  float64\n",
            " 13  Exited           33034 non-null  int64  \n",
            "dtypes: float64(5), int64(6), object(3)\n",
            "memory usage: 3.5+ MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 132000 entries, 0 to 131999\n",
            "Data columns (total 14 columns):\n",
            " #   Column           Non-Null Count   Dtype  \n",
            "---  ------           --------------   -----  \n",
            " 0   id               132000 non-null  int64  \n",
            " 1   CustomerId       132000 non-null  int64  \n",
            " 2   Surname          132000 non-null  object \n",
            " 3   CreditScore      132000 non-null  int64  \n",
            " 4   Geography        132000 non-null  object \n",
            " 5   Gender           132000 non-null  object \n",
            " 6   Age              132000 non-null  float64\n",
            " 7   Tenure           132000 non-null  int64  \n",
            " 8   Balance          132000 non-null  float64\n",
            " 9   NumOfProducts    132000 non-null  int64  \n",
            " 10  HasCrCard        132000 non-null  float64\n",
            " 11  IsActiveMember   132000 non-null  float64\n",
            " 12  EstimatedSalary  132000 non-null  float64\n",
            " 13  Exited           132000 non-null  int64  \n",
            "dtypes: float64(5), int64(6), object(3)\n",
            "memory usage: 14.1+ MB\n",
            "Mapping untuk kolom  non numerikSurname: {0: 'Abbie', 1: 'Abbott', 2: 'Abdullah', 3: 'Abdulov', 4: 'Abel', 5: 'Abernathy', 6: 'Abramov', 7: 'Abramova', 8: 'Abramovich', 9: 'Abrego', 10: 'Abron', 11: 'Achebe', 12: 'Adams', 13: 'Adamson', 14: 'Afamefula', 15: 'Afamefuna', 16: 'Afanasyeva', 17: 'Agafonova', 18: 'Aguirre', 19: 'Ahern', 20: 'Ahmed', 21: 'Aiken', 22: 'Aikenhead', 23: 'Ainsworth', 24: 'Aitken', 25: 'Ajuluchukwu', 26: 'Akabueze', 27: 'Akeroyd', 28: 'Akhtar', 29: 'Akobundu', 30: 'Aksakova', 31: 'Aksenov', 32: 'Aksenova', 33: 'Aksyonov', 34: 'Aksyonova', 35: 'Akubundu', 36: 'Akudinobi', 37: 'Alaniz', 38: 'Alderete', 39: 'Aldrich', 40: 'Aldridge', 41: 'Aleksandrova', 42: 'Alekseeva', 43: 'Alekseyeva', 44: 'Aleshire', 45: 'Alexander', 46: 'Alexandrov', 47: 'Alexandrova', 48: 'Alexeeva', 49: 'Alexeieva', 50: 'Alexeyeva', 51: 'Algarin', 52: 'Algeranoff', 53: 'Aliyeva', 54: 'Allan', 55: 'Allard', 56: 'Allardyce', 57: 'Allen', 58: 'Alley', 59: 'Alleyne', 60: 'Allingham', 61: 'Allnutt', 62: 'Allsop', 63: 'Alvares', 64: 'Alvarez', 65: 'Amadi', 66: 'Amaechi', 67: 'Amechi', 68: 'Amies', 69: 'Amos', 70: 'Ampt', 71: 'Anayochukwu', 72: 'Anayolisa', 73: 'Andersen', 74: 'Anderson', 75: 'Andreev', 76: 'Andrejew', 77: 'Andrews', 78: 'Andreyev', 79: 'Andreyeva', 80: 'Anenechi', 81: 'Anenechukwu', 82: 'Angel', 83: 'Angelo', 84: 'Ankudinov', 85: 'Ankudinova', 86: 'Ann', 87: 'Ansell', 88: 'Anthony', 89: 'Aparicio', 90: 'Arbour', 91: 'Archambault', 92: 'Archer', 93: 'Arcuri', 94: 'Ardis', 95: 'Arkwookerum', 96: 'Armfield', 97: 'Armstrong', 98: 'Arnold', 99: 'Arnott', 100: 'Arrington', 101: 'Artamonova', 102: 'Artemiev', 103: 'Artemieva', 104: 'Artemova', 105: 'Artemyeva', 106: 'Arthur', 107: 'Artyomova', 108: 'Ash', 109: 'Ashbolt', 110: 'Ashley', 111: 'Ashton', 112: 'Astorga', 113: 'Atherton', 114: 'Atkins', 115: 'Atkinson', 116: 'Austin', 117: 'Avdeev', 118: 'Avdeeva', 119: 'Avdeyeva', 120: 'Avent', 121: 'Averyanov', 122: 'Ayers', 123: 'Azarov', 124: 'Azikiwe', 125: 'Azubuike', 126: 'Azuka', 127: 'Babbage', 128: 'Baddeley', 129: 'Badgery', 130: 'Bage', 131: 'Bailey', 132: 'Bair', 133: 'Baird', 134: 'Baker', 135: 'Balashov', 136: 'Balashova', 137: 'Baldwin', 138: 'Bales', 139: 'Ball', 140: 'Ballard', 141: 'Balmain', 142: 'Balsillie', 143: 'Bancks', 144: 'Bancroft', 145: 'Band', 146: 'Banks', 147: 'Baranov', 148: 'Baranova', 149: 'Barber', 150: 'Barbour', 151: 'Barclay', 152: 'Barclay-Harvey', 153: 'Barese', 154: 'Baresi', 155: 'Barker', 156: 'Barling', 157: 'Barlow', 158: 'Barnard', 159: 'Barnes', 160: 'Barnet', 161: 'Barnett', 162: 'Barnhill', 163: 'Barrera', 164: 'Barrett', 165: 'Barry', 166: 'Bartlett', 167: 'Barton', 168: 'Baryshnikov', 169: 'Bateman', 170: 'Bates', 171: 'Bateson', 172: 'Batt', 173: 'Baxter', 174: 'Bayley', 175: 'Bazarova', 176: 'Bazhenov', 177: 'Bazile', 178: 'Beale', 179: 'Beam', 180: 'Bearce', 181: 'Beatham', 182: 'Beavers', 183: 'Becher', 184: 'Beck', 185: 'Becker', 186: 'Bednall', 187: 'Beede', 188: 'Begg', 189: 'Beggs', 190: 'Begley', 191: 'Begum', 192: 'Beit', 193: 'Belbin', 194: 'Belcher', 195: 'Belisario', 196: 'Bell', 197: 'Bellasis', 198: 'Bellew', 199: 'Bellucci', 200: 'Belonwu', 201: 'Belousov', 202: 'Belov', 203: 'Belstead', 204: 'Beluchi', 205: 'Beneventi', 206: 'Benford', 207: 'Benjamin', 208: 'Bennelong', 209: 'Bennet', 210: 'Bennett', 211: 'Bennetts', 212: 'Benson', 213: 'Bentley', 214: 'Bergamaschi', 215: 'Bergman', 216: 'Berkeley', 217: 'Bermudez', 218: 'Berry', 219: 'Bess', 220: 'Bevan', 221: 'Bevington', 222: 'Beyer', 223: 'Bezrukov', 224: 'Bezrukova', 225: 'Bianchi', 226: 'Bibi', 227: 'Bidencope', 228: 'Bidwill', 229: 'Billson', 230: 'Binder', 231: 'Bird', 232: 'Birdsall', 233: 'Birdseye', 234: 'Birk', 235: 'Birnie', 236: 'Biryukov', 237: 'Biryukova', 238: 'Bischof', 239: 'Bishop', 240: 'Bitter', 241: 'Black', 242: 'Blackall', 243: 'Blackburn', 244: 'Blackwood', 245: 'Blair', 246: 'Blake', 247: 'Blakey', 248: 'Bledsoe', 249: 'Blesing', 250: 'Bligh', 251: 'Blinova', 252: 'Blue', 253: 'Bluett', 254: 'Boag', 255: 'Boan', 256: 'Board', 257: 'Bobrov', 258: 'Bocharova', 259: 'Bochsa', 260: 'Bock', 261: 'Bogdanov', 262: 'Bogle', 263: 'Bogolyubov', 264: 'Bogolyubova', 265: 'Bold', 266: 'Bolton', 267: 'Boni', 268: 'Boniwell', 269: 'Bonwick', 270: 'Boone', 271: 'Booth', 272: 'Boothby', 273: 'Borchgrevink', 274: 'Bottrill', 275: 'Botts', 276: 'Boucaut', 277: 'Boulger', 278: 'Bovee', 279: 'Bowen', 280: 'Bowhay', 281: 'Bowman', 282: 'Boyd', 283: 'Boylan', 284: 'Boyle', 285: 'Bozeman', 286: 'Brabyn', 287: 'Bracewell', 288: 'Bradbury', 289: 'Bradley', 290: 'Bradshaw', 291: 'Brady', 292: 'Bray', 293: 'Brazenor', 294: 'Bremer', 295: 'Brenan', 296: 'Brennan', 297: 'Brewer', 298: 'Brient', 299: 'Brierly', 300: 'Briggs', 301: 'Brigstocke', 302: 'Brim', 303: 'Brizendine', 304: 'Broadhurst', 305: 'Bromby', 306: 'Bromley', 307: 'Bronner', 308: 'Brookes', 309: 'Brookman', 310: 'Brooks', 311: 'Brothers', 312: 'Brown', 313: 'Browne', 314: 'Brownless', 315: 'Brownlow', 316: 'Bruce', 317: 'Bruner', 318: 'Bruno', 319: 'Bryan', 320: 'Bryant', 321: 'Buccho', 322: 'Buchanan', 323: 'Buchi', 324: 'Buckland', 325: 'Buckley', 326: 'Buddicom', 327: 'Bufkin', 328: 'Builder', 329: 'Bukowski', 330: 'Buley', 331: 'Bulgakov', 332: 'Bull', 333: 'Bullen', 334: 'Bunton', 335: 'Burbidge', 336: 'Burdekin', 337: 'Burfitt', 338: 'Burgess', 339: 'Burgmann', 340: 'Burgos', 341: 'Burgoyne', 342: 'Burke', 343: 'Burlingame', 344: 'Burn', 345: 'Burns', 346: 'Burrows', 347: 'Burson', 348: 'Burt', 349: 'Burtch', 350: 'Burton', 351: 'Bushell', 352: 'Butcher', 353: 'Butler', 354: 'Buttenshaw', 355: 'Butters', 356: 'Butusov', 357: 'Bykov', 358: 'Byrne', 359: 'Caffyn', 360: 'Cairns', 361: 'Calabrese', 362: 'Calabresi', 363: 'Caldwell', 364: 'Calzada', 365: 'Cambage', 366: 'Cameron', 367: 'Campa', 368: 'Campbell', 369: 'Campos', 370: 'Candler', 371: 'Cantamessa', 372: 'Cantrell', 373: 'Capon', 374: 'Cardell', 375: 'Cardus', 376: 'Carey', 377: 'Carlson', 378: 'Carpenter', 379: 'Carr', 380: 'Carroll', 381: 'Carruthers', 382: 'Carslaw', 383: 'Carter', 384: 'Cartwright', 385: 'Cary', 386: 'Cashin', 387: 'Castella', 388: 'Castiglione', 389: 'Castillo', 390: 'Castles', 391: 'Castro', 392: 'Cattaneo', 393: 'Cavenagh', 394: 'Cawker', 395: 'Cawood', 396: 'Cawthorne', 397: 'Cayley', 398: 'Celis', 399: \"Ch'ang\", 400: \"Ch'en\", 401: \"Ch'eng\", 402: \"Ch'ien\", 403: \"Ch'in\", 404: \"Ch'iu\", 405: 'Challis', 406: 'Chamberlain', 407: 'Chamberlin', 408: 'Chambers', 409: 'Champion', 410: 'Chan', 411: 'Chandler', 412: 'Chang', 413: 'Chao', 414: 'Chapman', 415: 'Charlton', 416: 'Chase', 417: 'Chatfield', 418: 'Cheatham', 419: 'Chen', 420: 'Cheng', 421: 'Cherkasova', 422: 'Chesnokova', 423: 'Chester', 424: 'Chia', 425: 'Chiabuotu', 426: 'Chiagoziem', 427: 'Chialuka', 428: 'Chiang', 429: 'Chiawuotu', 430: 'Chiazagomekpele', 431: 'Chiazagomekpere', 432: 'Chibueze', 433: 'Chibugo', 434: 'Chibuzo', 435: 'Chidalu', 436: 'Chidi', 437: 'Chidiebele', 438: 'Chidiebere', 439: 'Chidiegwu', 440: 'Chidimma', 441: 'Chidozie', 442: 'Chidubem', 443: 'Chidumaga', 444: 'Chiebuka', 445: 'Chiedozie', 446: 'Chiefo', 447: 'Chiekwugo', 448: 'Chieloka', 449: 'Chiemeka', 450: 'Chiemela', 451: 'Chiemenam', 452: 'Chiemezie', 453: 'Chien', 454: 'Chienezie', 455: 'Chifley', 456: 'Chifo', 457: 'Chiganu', 458: 'Chigbogu', 459: 'Chigolum', 460: 'Chigozie', 461: 'Chijindum', 462: 'Chijioke', 463: 'Chikelu', 464: 'Chikere', 465: 'Chikezie', 466: 'Chikwado', 467: 'Chikwendu', 468: 'Childs', 469: 'Chill', 470: 'Chimaijem', 471: 'Chimaobim', 472: 'Chimaoke', 473: 'Chimaraoke', 474: 'Chimezie', 475: 'Chin', 476: 'Chinagorom', 477: 'Chinedum', 478: 'Chineze', 479: 'Chinomso', 480: 'Chinonyelum', 481: 'Chinweike', 482: 'Chinwemma', 483: 'Chinwendu', 484: 'Chinwenma', 485: 'Chinweuba', 486: 'Chioke', 487: 'Chiu', 488: 'Chiwetelu', 489: 'Chizoba', 490: 'Chizuoke', 491: 'Chou', 492: 'Christian', 493: 'Christie', 494: 'Christopher', 495: 'Chu', 496: 'Chuang', 497: 'Chubb', 498: 'Chukwualuka', 499: 'Chukwubuikem', 500: 'Chukwudi', 501: 'Chukwuebuka', 502: 'Chukwueloka', 503: 'Chukwuemeka', 504: 'Chukwufumnanya', 505: 'Chukwuhaenye', 506: 'Chukwujamuike', 507: 'Chukwujekwu', 508: 'Chukwukadibia', 509: 'Chukwukere', 510: 'Chukwuma', 511: 'Chukwumaobim', 512: 'Chukwunonso', 513: 'Chukwuraenye', 514: 'Chung', 515: 'Chuter', 516: 'Cisneros', 517: 'Claiborne', 518: 'Clamp', 519: 'Clancy', 520: 'Clapp', 521: 'Clark', 522: 'Clarke', 523: 'Claypool', 524: 'Clayton', 525: 'Clements', 526: 'Clendinnen', 527: 'Cleveland', 528: 'Clifton', 529: 'Clogstoun', 530: 'Clunie', 531: 'Coates', 532: 'Cobb', 533: 'Coburn', 534: 'Cocci', 535: 'Cochran', 536: 'Cockett', 537: 'Cockrum', 538: 'Cody', 539: 'Coffee', 540: 'Coffey', 541: 'Coffman', 542: 'Colbert', 543: 'Cole', 544: 'Colebatch', 545: 'Coleman', 546: 'Coles', 547: 'Collee', 548: 'Collier', 549: 'Collingridge de Tourcey', 550: 'Collins', 551: 'Colman', 552: 'Colombo', 553: 'Combes', 554: 'Combs', 555: 'Compton', 556: 'Cone', 557: 'Congreve', 558: 'Connely', 559: 'Connolly', 560: 'Connor', 561: 'Conti', 562: 'Converse', 563: 'Conway', 564: 'Cook', 565: 'Cooke', 566: 'Cookson', 567: 'Coombes', 568: 'Cooper', 569: 'Copeland', 570: 'Coppin', 571: 'Corbett', 572: 'Corones', 573: 'Corran', 574: 'Corrie', 575: 'Corser', 576: 'Corson', 577: 'Costa', 578: 'Cousens', 579: 'Cover', 580: 'Cowen', 581: 'Cowger', 582: 'Cox', 583: 'Crace', 584: 'Craig', 585: 'Craigie', 586: 'Crawford', 587: 'Cremin', 588: 'Cremonesi', 589: 'Creswell', 590: 'Cribb', 591: 'Crist', 592: 'Crocker', 593: 'Cross', 594: 'Crotty', 595: 'Crowther', 596: 'Crumbley', 597: 'Crump', 598: 'Cruz', 599: 'Culbreth', 600: 'Cullen', 601: 'Cumbrae-Stewart', 602: 'Cunningham', 603: 'Curnow', 604: 'Currey', 605: 'Curtis', 606: 'Custance', 607: 'Cyril', 608: 'Czajkowski', 609: \"D'Albertis\", 610: 'Dahlenburg', 611: 'Dale', 612: 'Dalrymple', 613: 'Dalton', 614: 'Daluchi', 615: 'Daly', 616: 'Dancy', 617: 'Daniels', 618: 'Dann', 619: 'Darling', 620: 'Darwin', 621: 'Davey', 622: 'David', 623: 'Davide', 624: 'Davidson', 625: 'Davies', 626: 'Davila', 627: 'Davis', 628: 'Davison', 629: 'Davy', 630: 'Davydova', 631: 'Dawkins', 632: 'Dawson', 633: 'Day', 634: 'De Bernales', 635: 'De Garis', 636: 'De Luca', 637: 'De Mestre', 638: 'De Neeve', 639: 'De Salis', 640: 'DeRose', 641: 'Dean', 642: 'Debellis', 643: 'Defalco', 644: 'Degtyarev', 645: 'Degtyaryov', 646: 'Deleon', 647: 'Dellucci', 648: 'Demaine', 649: 'Demidov', 650: 'Demuth', 651: 'Denisov', 652: 'Denisova', 653: 'Dennis', 654: 'Dennys', 655: 'Descoteaux', 656: 'Despeissis', 657: 'Dettmann', 658: 'Diaz', 659: 'Dickinson', 660: 'Dickson', 661: 'Diehl', 662: 'Dietz', 663: 'Diggs', 664: 'Dike', 665: 'Dilibe', 666: 'Dilke', 667: 'Dillon', 668: 'Dimauro', 669: 'Dipietro', 670: 'Diribe', 671: 'Disher', 672: 'Distefano', 673: 'Dixon', 674: 'Dobbs', 675: 'Dobie', 676: 'Dobson', 677: 'Docherty', 678: 'Dodd', 679: 'Dodds', 680: 'Dodgshun', 681: 'Doherty', 682: 'Dolgorukova', 683: 'Dominguez', 684: 'Donahue', 685: 'Donaldson', 686: 'Donnelly', 687: 'Donoghue', 688: 'Dore', 689: 'Douglas', 690: 'Downer', 691: 'Downie', 692: 'Dowse', 693: 'Doyle', 694: 'Doyne', 695: 'Drake-Brockman', 696: 'Drakeford', 697: 'Dreyer', 698: 'Drury', 699: 'Dubinina', 700: 'Duffy', 701: 'Dufresne', 702: 'Duggan', 703: 'Duigan', 704: 'Duke', 705: 'Dulhunty', 706: 'Dumetochukwu', 707: 'Dumetolisa', 708: 'Dumolo', 709: 'Dunbabin', 710: 'Duncan', 711: 'Dunn', 712: 'Dwyer', 713: 'Dyer', 714: 'Dynon', 715: 'Dyson', 716: 'Eames', 717: 'Earl', 718: 'Earle', 719: 'Ebelechukwu', 720: 'Ebelegbulam', 721: 'Eberechukwu', 722: 'Eberegbulam', 723: 'Echezonachukwu', 724: 'Ecuyer', 725: 'Eddy', 726: 'Edith', 727: 'Edments', 728: 'Edmondson', 729: 'Edmondstone', 730: 'Edmund la Touche', 731: 'Edwards', 732: 'Efimov', 733: 'Efremov', 734: 'Efremova', 735: 'Egobudike', 736: 'Eidson', 737: 'Eiland', 738: 'Eipper', 739: 'Ejikemeifeuwa', 740: 'Ejimofor', 741: 'Ekechukwu', 742: 'Ekwueme', 743: 'Elder', 744: 'Elewechi', 745: 'Elizabeth', 746: 'Elliot', 747: 'Elliott', 748: 'Ellis', 749: 'Elmore', 750: 'Eluemuno', 751: 'Emenike', 752: 'Emery', 753: 'Enderby', 754: 'Endrizzi', 755: 'Enemuo', 756: 'Enticknap', 757: 'Enyinnaya', 758: 'Eremenko', 759: 'Ermakov', 760: 'Ermakova', 761: 'Ershova', 762: 'Erskine', 763: 'Ervin', 764: 'Eskridge', 765: 'Esomchi', 766: 'Espinosa', 767: 'Esposito', 768: 'Esquivel', 769: 'Estep', 770: 'Estes', 771: 'Estrada', 772: 'Etheridge', 773: 'Eva', 774: 'Evans', 775: 'Evdokimov', 776: 'Everett', 777: 'Everingham', 778: 'Evseev', 779: 'Evseyev', 780: 'Ewen', 781: 'Ewers', 782: 'Ewing', 783: 'Fadden', 784: 'Fallaci', 785: 'Fan', 786: 'Fancher', 787: 'Fane', 788: 'Fang', 789: 'Fantin', 790: 'Fanucci', 791: 'Farber', 792: 'Faria', 793: 'Farmer', 794: 'Farnsworth', 795: 'Farrar', 796: 'Farrell', 797: 'Faulk', 798: 'Faulkner', 799: 'Favors', 800: 'Fedorov', 801: 'Fedorova', 802: 'Felix', 803: 'Feng', 804: 'Fennell', 805: 'Fennescey', 806: 'Fenton', 807: 'Ferdinand', 808: 'Ferguson', 809: 'Fernando', 810: 'Ferrari', 811: 'Ferreira', 812: 'Ferri', 813: 'Fetherstonhaugh', 814: 'Field', 815: 'Fielding', 816: 'Finch', 817: 'Findlay', 818: 'Fink', 819: 'Fiore', 820: 'Fiorentini', 821: 'Fiorentino', 822: 'Fischer', 823: 'Fisher', 824: 'Fisk', 825: 'Fitch', 826: 'Fitts', 827: 'Fitzgerald', 828: 'Fitzpatrick', 829: 'Flannagan', 830: 'Flannery', 831: 'Fleetwood-Smith', 832: 'Fleming', 833: 'Flemming', 834: 'Fletcher', 835: 'Flores', 836: 'Floyd', 837: 'Flynn', 838: 'Fokina', 839: 'Fokine', 840: 'Foley', 841: 'Folliero', 842: 'Fomin', 843: 'Fomina', 844: 'Fontaine', 845: 'Fontenot', 846: 'Forbes', 847: 'Ford', 848: 'Forlonge', 849: 'Forster', 850: 'Forwood', 851: 'Foster', 852: 'Foveaux', 853: 'Fowler', 854: 'Fox', 855: 'Foxall', 856: 'Francis', 857: 'Franklin', 858: 'Franz', 859: 'Fraser', 860: 'Frater', 861: 'Frederick', 862: 'Frederickson', 863: 'Freeman', 864: 'French', 865: 'Frewin', 866: 'Fries', 867: 'Froggatt', 868: 'Frolov', 869: 'Frolova', 870: 'Frost', 871: 'Frye', 872: 'Fu', 873: 'Fulks', 874: 'Fuller', 875: 'Fullwood', 876: 'Furneaux', 877: 'Fyans', 878: 'Fyodorov', 879: 'Fyodorova', 880: 'Gadsden', 881: 'Gadsdon', 882: 'Gaffney', 883: 'Galgano', 884: 'Galkin', 885: 'Galkina', 886: 'Gallagher', 887: 'Gallo', 888: 'Gallop', 889: 'Gamble', 890: 'Gambrell', 891: 'Game', 892: 'Gannon', 893: 'Gant', 894: 'Garcia', 895: 'Gardener', 896: 'Gardiner', 897: 'Gardner', 898: 'Garland', 899: 'Garmon', 900: 'Garner', 901: 'Garnsey', 902: 'Garrett', 903: 'Gartrell', 904: 'Gay', 905: 'Gboliwe', 906: 'Geach', 907: 'Gearhart', 908: 'Gearheart', 909: 'Gebhart', 910: 'Gell', 911: 'Genovese', 912: 'Genovesi', 913: 'Gentry', 914: 'Geoghegan', 915: 'Georg', 916: 'George', 917: 'Gerald', 918: 'Gerasimov', 919: 'Gerasimova', 920: 'Gether', 921: 'Gibbons', 922: 'Gibbs', 923: 'Gibson', 924: 'Gidney', 925: 'Gilbert', 926: 'Gilchrist', 927: 'Giles', 928: 'Gill', 929: 'Gilleland', 930: 'Gilroy', 931: 'Ginikanwa', 932: 'Ginn', 933: 'Giordano', 934: 'Glasgow', 935: 'Glassman', 936: 'Glazkov', 937: 'Gleeson', 938: 'Glennon', 939: 'Glenny', 940: 'Glossop', 941: 'Glover', 942: 'Goddard', 943: 'Godfrey', 944: 'Goering', 945: 'Goforth', 946: 'Golibe', 947: 'Goliwe', 948: 'Goloubev', 949: 'Golovanov', 950: 'Golubev', 951: 'Golubeva', 952: 'Golubov', 953: 'Golubova', 954: 'Gomes', 955: 'Gonzalez', 956: 'Goodman', 957: 'Goodwin', 958: 'Gorbunov', 959: 'Gorbunova', 960: 'Gordon', 961: 'Gorman', 962: 'Gorshkov', 963: 'Gosnell', 964: 'Gotch', 965: 'Gouger', 966: 'Gough', 967: 'Gould', 968: 'Gow', 969: 'Graham', 970: 'Grant', 971: 'Gratton', 972: 'Gratwick', 973: 'Grave', 974: 'Gray', 975: 'Greathouse', 976: 'Greaves', 977: 'Greco', 978: 'Greece', 979: 'Green', 980: 'Greene', 981: 'Greenhalgh', 982: 'Greenwood', 983: 'Gregory', 984: 'Gregson', 985: 'Gresswell', 986: 'Grieve', 987: 'Griffen', 988: 'Griffin', 989: 'Griffiths', 990: 'Grigoryeva', 991: 'Grimmett', 992: 'Groom', 993: 'Grosse', 994: 'Grover', 995: 'Groves', 996: 'Grubb', 997: 'Guerin', 998: 'Guerra', 999: 'Gunson', 1000: 'Gunter', 1001: 'H?', 1002: 'Hackett', 1003: 'Haddon', 1004: 'Hagins', 1005: 'Hairston', 1006: 'Hale', 1007: 'Hales', 1008: 'Hall', 1009: 'Hallahan', 1010: 'Halpern', 1011: 'Ham', 1012: 'Hamilton', 1013: 'Hammer', 1014: 'Hammond', 1015: 'Hammonds', 1016: 'Hampton', 1017: 'Han', 1018: 'Hancock', 1019: 'Hand', 1020: 'Hanna', 1021: 'Hannah', 1022: 'Hansen', 1023: 'Hanson', 1024: 'Hao', 1025: 'Hardacre', 1026: 'Hardiman', 1027: 'Harding', 1028: 'Hardy', 1029: 'Hare', 1030: 'Harewood', 1031: 'Hargrave', 1032: 'Hargraves', 1033: 'Hargreaves', 1034: 'Harker', 1035: 'Harper', 1036: 'Harrell', 1037: 'Harriman', 1038: 'Harrington', 1039: 'Harris', 1040: 'Harrison', 1041: 'Hart', 1042: 'Hartley', 1043: 'Hartung', 1044: 'Hartzler', 1045: 'Harvey', 1046: 'Hassall', 1047: 'Haugh', 1048: 'Hawdon', 1049: 'Hawes', 1050: 'Hawkins', 1051: 'Hawks', 1052: 'Haworth', 1053: 'Hawthorn', 1054: 'Hay', 1055: 'Hayden', 1056: 'Hayes', 1057: 'Hayes-Williams', 1058: 'Haynes', 1059: 'Hayslett', 1060: 'Hayward', 1061: 'Hazon', 1062: 'He', 1063: 'Heap', 1064: 'Heard', 1065: 'Hearn', 1066: 'Heath', 1067: 'Hebert', 1068: 'Helena', 1069: 'Heller', 1070: 'Henderson', 1071: 'Hendley', 1072: 'Hendrick', 1073: 'Henry', 1074: 'Henty', 1075: 'Herbert', 1076: 'Hernandez', 1077: 'Herrera', 1078: 'Herrin', 1079: 'Herz', 1080: 'Hess', 1081: 'Hewitt', 1082: 'Heydon', 1083: 'Hibbins', 1084: 'Hickey', 1085: 'Hicks', 1086: 'Higgins', 1087: 'Highett', 1088: 'Highland', 1089: 'Hightower', 1090: 'Hill', 1091: 'Hilton', 1092: 'Hingston', 1093: 'Hinton', 1094: 'Hirst', 1095: 'Hixson', 1096: 'Ho', 1097: 'Hobbs', 1098: 'Hobler', 1099: 'Hobson', 1100: 'Hodge', 1101: 'Hodgson', 1102: 'Hoelscher', 1103: 'Holbrook', 1104: 'Holden', 1105: 'Holder', 1106: 'Holland', 1107: 'Hollis', 1108: 'Holloway', 1109: 'Holman', 1110: 'Holmes', 1111: 'Holmwood', 1112: 'Holt', 1113: 'Honore', 1114: 'Hooker', 1115: 'Hoolan', 1116: 'Hooper', 1117: 'Hope', 1118: 'Hopetoun', 1119: 'Hopkins', 1120: 'Hopman', 1121: 'Hopwood', 1122: 'Horan', 1123: 'Hornung', 1124: 'Horsfall', 1125: 'Horsley', 1126: 'Hort', 1127: 'Horton', 1128: 'Hotchin', 1129: 'Hou', 1130: 'Houghton', 1131: 'Hovell', 1132: 'Howard', 1133: 'Howarde', 1134: 'Howarth', 1135: 'Howe', 1136: 'Howell', 1137: 'Howell-Price', 1138: 'Howells', 1139: 'Howey', 1140: 'Hs?', 1141: 'Hs?eh', 1142: 'Hsia', 1143: 'Hsiao', 1144: 'Hsieh', 1145: 'Hsing', 1146: 'Hsiung', 1147: 'Hsu', 1148: 'Hsueh', 1149: 'Hu', 1150: 'Huang', 1151: 'Huddart', 1152: 'Hudson', 1153: 'Huggins', 1154: 'Hughes', 1155: 'Hughes-Jones', 1156: 'Huguley', 1157: 'Huie', 1158: 'Hull', 1159: 'Humphreys', 1160: 'Humphries', 1161: 'Hung', 1162: 'Hunt', 1163: 'Hunter', 1164: 'Hurst', 1165: 'Hussain', 1166: 'Hussey', 1167: 'Hutcheon', 1168: 'Hutchinson', 1169: 'Hyde', 1170: 'Hysell', 1171: 'Iadanza', 1172: 'Ibbott', 1173: 'Ibeabuchi', 1174: 'Ibeamaka', 1175: 'Ibekwe', 1176: 'Ibezimako', 1177: 'Ibragimova', 1178: 'Ibrahimov', 1179: 'Ibrahimova', 1180: 'Ifeajuna', 1181: 'Ifeanacho', 1182: 'Ifeanyichukwu', 1183: 'Ifeatu', 1184: 'Ifesinachi', 1185: 'Ignatieff', 1186: 'Ignatiev', 1187: 'Ignatyev', 1188: 'Ignatyeva', 1189: 'Igwebuike', 1190: 'Iheanacho', 1191: 'Iheatu', 1192: 'Ijendu', 1193: 'Ikechukwu', 1194: 'Ikedinachukwu', 1195: 'Ikemefuna', 1196: 'Ikenna', 1197: 'Illingworth', 1198: 'Iloabuchi', 1199: 'Iloerika', 1200: 'Ilyina', 1201: 'Ingamells', 1202: 'Ingle', 1203: 'Ingram', 1204: 'Ingrassia', 1205: 'Inman', 1206: 'Innes', 1207: 'Iqbal', 1208: 'Iredale', 1209: 'Ireland', 1210: 'Iroawuchi', 1211: 'Isaacs', 1212: 'Isayev', 1213: 'Isayeva', 1214: 'Istomin', 1215: 'Ives', 1216: 'Iweobiegbulam', 1217: 'Iweobiegbunam', 1218: 'Izmailova', 1219: 'Izuchukwu', 1220: 'Jacka', 1221: 'Jackson', 1222: 'James', 1223: 'Jamieson', 1224: 'Jamison', 1225: 'Jara', 1226: 'Jarvis', 1227: 'Jefferies', 1228: 'Jefferson', 1229: 'Jeffrey', 1230: 'Jen', 1231: 'Jenkins', 1232: 'Jenks', 1233: 'Jennings', 1234: 'Jensen', 1235: 'Jess', 1236: 'Jessop', 1237: 'Jibunoh', 1238: 'Jideofor', 1239: 'Jimenez', 1240: 'Jobson', 1241: 'John', 1242: 'Johnson', 1243: 'Johnston', 1244: 'Johnstone', 1245: 'Jolly', 1246: 'Jonathan', 1247: 'Jones', 1248: 'Jordan', 1249: 'Jose', 1250: 'Joseph', 1251: 'Joshua', 1252: 'Joslin', 1253: 'Jowers', 1254: 'Jowett', 1255: 'Judd', 1256: 'Jude', 1257: \"K'ung\", 1258: 'K?', 1259: 'Kable', 1260: 'Kaeppel', 1261: 'Kaleski', 1262: 'Kalinina', 1263: 'Kambinachi', 1264: 'Kanayochukwu', 1265: 'Kane', 1266: 'Kang', 1267: 'Kao', 1268: 'Kaodilinakachukwu', 1269: 'Kapustin', 1270: 'Kapustina', 1271: 'Kashiwagi', 1272: 'Kauffmann', 1273: 'Kaur', 1274: 'Kay', 1275: 'Kazakova', 1276: 'Kazantsev', 1277: 'Kazantseva', 1278: 'Keane', 1279: 'Keating', 1280: 'Keeley', 1281: 'Keen', 1282: 'Kegley', 1283: 'Keldie', 1284: 'Kelechi', 1285: 'Kelley', 1286: 'Kelly', 1287: 'Kemp', 1288: 'Kendall', 1289: 'Kenechi', 1290: 'Kenechukwu', 1291: 'Kenenna', 1292: 'Kennedy', 1293: 'Kenniff', 1294: 'Kent', 1295: 'Kentish', 1296: 'Kepley', 1297: 'Kerr', 1298: 'Kershaw', 1299: 'Kesteven', 1300: 'Khan', 1301: 'Kharlamov', 1302: 'Kharlamova', 1303: 'Kibble', 1304: 'Kibby', 1305: 'Kiernan', 1306: 'Kilgour', 1307: 'Kincaid', 1308: 'Kinder', 1309: 'King', 1310: 'Kingsley', 1311: 'Kinlaw', 1312: 'Kirby', 1313: 'Kirillov', 1314: 'Kirillova', 1315: 'Kirk', 1316: 'Kirkland', 1317: 'Kirsova', 1318: 'Kirwan', 1319: 'Kisch', 1320: 'Kistler', 1321: 'Klein', 1322: 'Kline', 1323: 'Knepper', 1324: 'Knight', 1325: 'Knipe', 1326: 'Knorr', 1327: 'Knowles', 1328: 'Knox', 1329: 'Knupp', 1330: 'Ko', 1331: 'Koch', 1332: 'Kodilinyechukwu', 1333: 'Koehler', 1334: 'Koger', 1335: 'Kolesnikov', 1336: 'Kolesnikova', 1337: 'Komar', 1338: 'Komarova', 1339: 'Konovalova', 1340: 'Koo', 1341: 'Kornilova', 1342: 'Korovin', 1343: 'Korovina', 1344: 'Kosisochukwu', 1345: 'Kovalev', 1346: 'Kovaleva', 1347: 'Kovalyov', 1348: 'Kovalyova', 1349: 'Kozlova', 1350: 'Kramer', 1351: 'Krawczyk', 1352: 'Krichauff', 1353: 'Krischock', 1354: 'Kruglov', 1355: 'Kruglova', 1356: 'Krylov', 1357: 'Kryukova', 1358: 'Ku', 1359: 'Kudryashova', 1360: 'Kulikova', 1361: 'Kumm', 1362: 'Kung', 1363: 'Kuo', 1364: 'Kuykendall', 1365: 'Kuznetsova', 1366: 'Kwemto', 1367: 'Kwemtochukwu', 1368: 'L?', 1369: 'Lablanc', 1370: 'Labrador', 1371: 'Lafleur', 1372: 'Lahti', 1373: 'Lai', 1374: 'Laidley', 1375: 'Lajoie', 1376: 'Lamb', 1377: 'Lambert', 1378: 'Lambie', 1379: 'Lamble', 1380: 'Lampungmeiua', 1381: 'Landman', 1382: 'Landor', 1383: 'Landry', 1384: 'Landseer', 1385: 'Lane', 1386: 'Laney', 1387: 'Lanford', 1388: 'Lang', 1389: 'Langdon', 1390: 'Langlands', 1391: 'Langler', 1392: 'Lappin', 1393: 'Larionova', 1394: 'Larkin', 1395: 'Larsen', 1396: 'Larson', 1397: 'Lascelles', 1398: 'Lassetter', 1399: 'Lattimore', 1400: 'Laurie', 1401: 'Lavarack', 1402: 'Lavine', 1403: 'Lavrentiev', 1404: 'Lavrov', 1405: 'Law', 1406: 'Lawless', 1407: 'Lawley', 1408: 'Lawrence', 1409: 'Lawson', 1410: 'Lay', 1411: 'Layh', 1412: 'Lazar', 1413: 'Lazarev', 1414: 'Lazareva', 1415: 'Le Gallienne', 1416: 'Le Grand', 1417: 'Le Hunte', 1418: 'Leach', 1419: 'Leak', 1420: 'Lear', 1421: 'Learmonth', 1422: 'Leckie', 1423: 'Lederer', 1424: 'Lee', 1425: 'Lees', 1426: 'Lehr', 1427: 'Lei', 1428: 'Leibius', 1429: 'Lenhardt', 1430: 'Lennox', 1431: 'Leonard', 1432: 'Lettiere', 1433: 'Levan', 1434: 'Levi', 1435: 'Levien', 1436: 'Levy', 1437: 'Lewis', 1438: 'Leworthy', 1439: 'Li', 1440: 'Li Fonti', 1441: 'Liang', 1442: 'Liao', 1443: 'Liardet', 1444: 'Liebe', 1445: 'Light', 1446: 'Lilly', 1447: 'Lin', 1448: 'Lindell', 1449: 'Lindon', 1450: 'Lindsay', 1451: 'Linger', 1452: 'Linton', 1453: 'Lipton', 1454: 'Lira', 1455: 'Little', 1456: 'Liu', 1457: 'Lloyd', 1458: 'Lo', 1459: 'Lo Duca', 1460: 'Loane', 1461: 'Lock', 1462: 'Locke', 1463: 'Lockett', 1464: 'Lockington', 1465: 'Lockyer', 1466: 'Loewenthal', 1467: 'Loftus', 1468: 'Logan', 1469: 'Loggia', 1470: 'Loginov', 1471: 'Logue', 1472: 'Lombardi', 1473: 'Lombardo', 1474: 'Long', 1475: 'Longo', 1476: 'Longstaff', 1477: 'Lopez', 1478: 'Lord', 1479: 'Lorenzo', 1480: 'Lori', 1481: 'Loton', 1482: 'Louis', 1483: 'Lovely', 1484: 'Loving', 1485: 'Lowe', 1486: 'Lowell', 1487: 'Lowrie', 1488: 'Loyau', 1489: 'Lu', 1490: 'Lucas', 1491: 'Lucchese', 1492: 'Lucchesi', 1493: 'Lucciano', 1494: 'Ludowici', 1495: 'Lueck', 1496: 'Luffman', 1497: 'Lujan', 1498: 'Lumholtz', 1499: 'Lung', 1500: 'Lupton', 1501: 'Lynch', 1502: 'Lynton', 1503: 'Lysaght', 1504: 'Ma', 1505: 'MacDevitt', 1506: 'MacDonald', 1507: 'MacDonnell', 1508: 'MacPherson', 1509: 'Macadam', 1510: 'Macarthur', 1511: 'Macartney', 1512: 'Maccallum', 1513: 'Macdonald', 1514: 'Macfarlan', 1515: 'Macgroarty', 1516: 'Mach', 1517: 'Macintyre', 1518: 'Mack', 1519: 'Mackay', 1520: 'Mackenzie', 1521: 'Mackey', 1522: 'Mackie', 1523: 'Mackinlay', 1524: 'Macknight', 1525: 'Maclean', 1526: 'Macleod', 1527: 'Macnamara', 1528: 'Maconochie', 1529: 'Macrossan', 1530: 'Mactier', 1531: 'Macvitie', 1532: 'Madison', 1533: 'Maduabuchim', 1534: 'Madueke', 1535: 'Madukaego', 1536: 'Madukaife', 1537: 'Madukwe', 1538: 'Maggard', 1539: 'Maher', 1540: 'Mahmood', 1541: 'Mahon', 1542: 'Mai', 1543: 'Mairinger', 1544: 'Maitland', 1545: 'Major', 1546: 'Malloy', 1547: 'Mamelu', 1548: 'Mancini', 1549: 'Manfrin', 1550: 'Mann', 1551: 'Manna', 1552: 'Manners', 1553: 'Manning', 1554: 'Mao', 1555: 'Marcelo', 1556: 'Marchesi', 1557: 'Marcum', 1558: 'Marcus', 1559: 'Marian', 1560: 'Marino', 1561: 'Mario', 1562: 'Marks', 1563: 'Marrero', 1564: 'Marsden', 1565: 'Marsh', 1566: 'Marshall', 1567: 'Marshall-Hall', 1568: 'Martel', 1569: 'Martin', 1570: 'Martinez', 1571: 'Maruff', 1572: 'Mashman', 1573: 'Maslov', 1574: 'Maslova', 1575: 'Maslow', 1576: 'Mason', 1577: 'Massie', 1578: 'Mathews', 1579: 'Matlock', 1580: 'Matthews', 1581: 'Matthias', 1582: 'Matveyev', 1583: 'Matveyeva', 1584: 'Maughan', 1585: 'Mauldon', 1586: 'Mault', 1587: 'Maurer', 1588: 'Maxwell', 1589: 'May', 1590: 'Maynard', 1591: 'Mayne', 1592: 'Mayrhofer', 1593: 'Mays', 1594: 'Mazure', 1595: 'Mazzanti', 1596: 'Mazzi', 1597: 'Mbadiwe', 1598: 'Mbanefo', 1599: 'McBurney', 1600: 'McCaffrey', 1601: 'McCall', 1602: 'McCane', 1603: 'McCardle', 1604: 'McCarthy', 1605: 'McCartney', 1606: 'McCawley', 1607: 'McClemans', 1608: 'McClinton', 1609: 'McConnell', 1610: 'McCulloch', 1611: 'McDavid', 1612: 'McDonald', 1613: 'McElhone', 1614: 'McElroy', 1615: 'McElyea', 1616: 'McEncroe', 1617: 'McEwan', 1618: 'McFarland', 1619: 'McGarry', 1620: 'McGill', 1621: 'McGregor', 1622: 'McGuigan', 1623: 'McGuirk', 1624: 'McIntosh', 1625: 'McIntyre', 1626: 'McIver', 1627: 'McKay', 1628: 'McKee', 1629: 'McKelvey', 1630: 'McKenzie', 1631: 'McKinley', 1632: 'McKinnon', 1633: 'McKissick', 1634: 'McLachlan', 1635: 'McLean', 1636: 'McMasters', 1637: 'McMillan', 1638: 'McMorran', 1639: 'McNaughtan', 1640: 'McNeil', 1641: 'McNeill', 1642: 'McNess', 1643: 'McVey', 1644: 'McWilliam', 1645: 'McWilliams', 1646: 'Mead', 1647: 'Meagher', 1648: 'Meany', 1649: 'Medvedev', 1650: 'Medvedeva', 1651: 'Meldrum', 1652: 'Melendez', 1653: 'Mello', 1654: 'Mellor', 1655: 'Melton', 1656: 'Melvin', 1657: 'Mendes', 1658: 'Meng', 1659: 'Menhennitt', 1660: 'Menkens', 1661: 'Merrett', 1662: 'Messersmith', 1663: 'Metcalf', 1664: 'Metcalfe', 1665: 'Miah', 1666: 'Micco', 1667: 'Michael', 1668: 'Michel', 1669: 'Michelides', 1670: 'Mickey', 1671: 'Micklem', 1672: 'Middleton', 1673: 'Mikkelsen', 1674: 'Milanesi', 1675: 'Milani', 1676: 'Milano', 1677: 'Miles', 1678: 'Millar', 1679: 'Miller', 1680: 'Milligan', 1681: 'Milliner', 1682: 'Mills', 1683: 'Milne', 1684: 'Miracle', 1685: 'Mirams', 1686: 'Miranda', 1687: 'Mironov', 1688: 'Mironova', 1689: 'Mishin', 1690: 'Mishina', 1691: 'Mistry', 1692: 'Mitchell', 1693: 'Moen', 1694: 'Moffitt', 1695: 'Molineux', 1696: 'Molle', 1697: 'Mollison', 1698: 'Monaldo', 1699: 'Monds', 1700: 'Montalvo', 1701: 'Montemayor', 1702: 'Montes', 1703: 'Montgomery', 1704: 'Moody', 1705: 'Moon', 1706: 'Moore', 1707: 'Morales', 1708: 'Moran', 1709: 'Morant', 1710: 'Mordvinova', 1711: 'More', 1712: 'Morehead', 1713: 'Moreno', 1714: 'Moretti', 1715: 'Morey', 1716: 'Morgan', 1717: 'Morin', 1718: 'Morley', 1719: 'Moroney', 1720: 'Morres', 1721: 'Morrice', 1722: 'Morris', 1723: 'Morrison', 1724: 'Mort', 1725: 'Moseley', 1726: 'Mosley', 1727: 'Mosman', 1728: 'Moss', 1729: 'Mott', 1730: 'Moyes', 1731: 'Moysey', 1732: 'Muecke', 1733: 'Mueller', 1734: 'Muir', 1735: 'Mullah', 1736: 'Mullan', 1737: 'Mullawirraburka', 1738: 'Mundy', 1739: 'Munro', 1740: 'Munroe', 1741: 'Munson', 1742: 'Munz', 1743: 'Muomelu', 1744: 'Muravyov', 1745: 'Muravyova', 1746: 'Murphy', 1747: 'Murray', 1748: 'Muse', 1749: 'Musgrove', 1750: 'Myers', 1751: 'Nakayama', 1752: 'Namatjira', 1753: 'Napolitani', 1754: 'Napolitano', 1755: 'Naquin', 1756: 'Narelle', 1757: 'Nash', 1758: 'Navarrete', 1759: 'Naylor', 1760: 'Ndubuagha', 1761: 'Ndubueze', 1762: 'Ndubuisi', 1763: 'Ndukaku', 1764: 'Neal', 1765: 'Nebechi', 1766: 'Nebechukwu', 1767: 'Nebeolisa', 1768: 'Nebeuwa', 1769: 'Neitenstein', 1770: 'Nekrasov', 1771: 'Nekrasova', 1772: 'Nelson', 1773: 'Nepean', 1774: 'Nero', 1775: 'Neumann', 1776: 'Neumayer', 1777: 'Nevels', 1778: 'Nevzorova', 1779: 'Newbery', 1780: 'Newbold', 1781: 'Newland', 1782: 'Newsom', 1783: 'Newton', 1784: 'Ngozichukwuka', 1785: 'Ni', 1786: 'Nicholls', 1787: 'Nicholson', 1788: 'Nickson', 1789: 'Nicoll', 1790: 'Niehaus', 1791: 'Nielson', 1792: 'Nieves', 1793: 'Nikitina', 1794: 'Nina', 1795: 'Niu', 1796: 'Nixon', 1797: 'Nkemakolam', 1798: 'Nkemakonam', 1799: 'Nkemdilim', 1800: 'Nkemdirim', 1801: 'Nkemjika', 1802: 'Nnabuife', 1803: 'Nnachetam', 1804: 'Nnaemeka', 1805: 'Nnaife', 1806: 'Nnamdi', 1807: 'Nnamutaezinwa', 1808: 'Nnanna', 1809: 'Nnonso', 1810: 'Noble', 1811: 'Nock', 1812: 'Nolan', 1813: 'Norman', 1814: 'Norris', 1815: 'North', 1816: 'Northern', 1817: 'Northey', 1818: 'Norton', 1819: 'Nott', 1820: 'Nucci', 1821: 'Nuttall', 1822: 'Nwabugwu', 1823: 'Nwachinemelu', 1824: 'Nwachukwu', 1825: 'Nwagugheuzo', 1826: 'Nwankwo', 1827: 'Nwebube', 1828: 'Nweke', 1829: 'Nwokeocha', 1830: 'Nwokezuike', 1831: 'Nwokike', 1832: 'Nwora', 1833: 'Nworie', 1834: 'Nwoye', 1835: 'Nyhan', 1836: \"O'Brien\", 1837: \"O'Callaghan\", 1838: \"O'Connor\", 1839: \"O'Donnell\", 1840: \"O'Kane\", 1841: \"O'Loghlen\", 1842: \"O'Loghlin\", 1843: \"O'Loughlin\", 1844: \"O'Meara\", 1845: \"O'Neill\", 1846: \"O'Sullivan\", 1847: \"O'Toole\", 1848: 'Obiajulu', 1849: 'Obialo', 1850: 'Obidimkpa', 1851: 'Obielumani', 1852: 'Obijiaku', 1853: 'Obinna', 1854: 'Obioma', 1855: 'Obiora', 1856: 'Obiuto', 1857: 'Ochoa', 1858: 'Odell', 1859: 'Odili', 1860: 'Odinakachukwu', 1861: 'Ofodile', 1862: 'Ogbonnaya', 1863: 'Ogg', 1864: 'Ogle', 1865: 'Ogochukwu', 1866: 'Oguejiofor', 1867: 'Ohearn', 1868: 'Ojiofor', 1869: 'Okagbue', 1870: 'Okechukwu', 1871: 'Okeke', 1872: 'Okoli', 1873: 'Okonkwo', 1874: 'Okorie', 1875: 'Okwuadigbo', 1876: 'Okwudilichukwu', 1877: 'Okwudiliolisa', 1878: 'Okwukwe', 1879: 'Okwuoma', 1880: 'Olague', 1881: 'Oldham', 1882: 'Oleary', 1883: 'Olejuru', 1884: 'Olisaemeka', 1885: 'Olisanugo', 1886: 'Oliver', 1887: 'Olsen', 1888: 'Olson', 1889: 'Olszewski', 1890: 'Oluchi', 1891: 'Oluchukwu', 1892: 'Omeokachie', 1893: 'Onio', 1894: 'Onochie', 1895: 'Onodugoadiegbemma', 1896: 'Onuchukwu', 1897: 'Onuoha', 1898: 'Onuora', 1899: 'Onwuamaegbu', 1900: 'Onwuamaeze', 1901: 'Onwuatuegwu', 1902: 'Onwubiko', 1903: 'Onwudiwe', 1904: 'Onwuemelie', 1905: 'Onwughara', 1906: 'Onwuka', 1907: 'Onwumelu', 1908: 'Onyekachi', 1909: 'Onyekachukwu', 1910: 'Onyekaozulu', 1911: 'Onyemachukwu', 1912: 'Onyemaechi', 1913: 'Onyemauchechi', 1914: 'Onyemauchechukwu', 1915: 'Onyemere', 1916: 'Onyenachiya', 1917: 'Onyeorulu', 1918: 'Onyeoruru', 1919: 'Onyinyechukwuka', 1920: 'Osborne', 1921: 'Osinachi', 1922: 'Ositadimma', 1923: 'Osonduagwuike', 1924: 'Oster', 1925: 'Otitodilichukwu', 1926: 'Otitodilinna', 1927: 'Otoole', 1928: 'Otutodilichukwu', 1929: 'Otutodilinna', 1930: 'Outhwaite', 1931: 'Outlaw', 1932: 'Outtrim', 1933: 'Overby', 1934: 'Owen', 1935: 'Owens', 1936: 'Ozerova', 1937: 'Ozioma', 1938: 'Ozoemena', 1939: 'Ozuluonye', 1940: \"P'an\", 1941: \"P'eng\", 1942: 'Pacheco', 1943: 'Padilla', 1944: 'Padovano', 1945: 'Padovesi', 1946: 'Pagan', 1947: 'Page', 1948: 'Pagnotto', 1949: 'Pai', 1950: 'Palazzi', 1951: 'Palerma', 1952: 'Palermo', 1953: 'Palfreyman', 1954: 'Paling', 1955: 'Palmer', 1956: 'Palmerston', 1957: 'Pan', 1958: 'Panicucci', 1959: 'Panina', 1960: 'Pankhurst', 1961: 'Pape', 1962: 'Paramor', 1963: 'Pardey', 1964: 'Parker', 1965: 'Parkes', 1966: 'Parkhill', 1967: 'Parkin', 1968: 'Parkinson', 1969: 'Parks', 1970: 'Parrott', 1971: 'Parry', 1972: 'Parry-Okeden', 1973: 'Parsons', 1974: 'Patel', 1975: 'Paten', 1976: 'Paterson', 1977: 'Patrick', 1978: 'Patterson', 1979: 'Pauley', 1980: 'Pavlova', 1981: 'Payne', 1982: 'Payton', 1983: 'Peacock', 1984: 'Pearce', 1985: 'Pearson', 1986: 'Peavy', 1987: 'Peck', 1988: 'Pedder', 1989: 'Peel', 1990: 'Pendergrass', 1991: 'Pendred', 1992: 'Peng', 1993: 'Pennington', 1994: 'Pepper', 1995: 'Peppin', 1996: 'Percy', 1997: 'Perez', 1998: 'Perkin', 1999: 'Perkins', 2000: 'Perrodin', 2001: 'Perry', 2002: 'Pethard', 2003: 'Pettit', 2004: 'Pettry', 2005: 'Peyser', 2006: 'Pham', 2007: 'Pharr', 2008: 'Phelan', 2009: 'Philip', 2010: 'Philipp', 2011: 'Phillipps', 2012: 'Phillips', 2013: 'Piazza', 2014: 'Piccio', 2015: 'Pickering', 2016: 'Pickworth', 2017: 'Picot', 2018: 'Pike', 2019: 'Pino', 2020: 'Pinto', 2021: 'Pipes', 2022: 'Pirogov', 2023: 'Pirogova', 2024: 'Pirozzi', 2025: 'Pisani', 2026: 'Pisano', 2027: 'Pitcher', 2028: 'Pitts', 2029: 'Plant', 2030: 'Plascencia', 2031: 'Plumb', 2032: 'Plummer', 2033: 'Pokrovskaya', 2034: 'Pokrovskii', 2035: 'Pokrovsky', 2036: 'Pollard', 2037: 'Polyakov', 2038: 'Polyakova', 2039: 'Pomeroy', 2040: 'Ponce', 2041: 'Poninski', 2042: 'Ponomarev', 2043: 'Ponomaryov', 2044: 'Ponomaryova', 2045: 'Poole', 2046: 'Pope', 2047: 'Porter', 2048: 'Potter', 2049: 'Pottinger', 2050: 'Potts', 2051: 'Powell', 2052: 'Power', 2053: 'Pratt', 2054: 'Preston', 2055: 'Price', 2056: 'Priestley', 2057: 'Pritchard', 2058: 'Prokhorova', 2059: 'Pruneda', 2060: 'Pugh', 2061: 'Pugliesi', 2062: 'Purdy', 2063: 'Pye', 2064: 'Quaife', 2065: 'Quesada', 2066: 'Quinn', 2067: 'Quinones', 2068: 'Radcliffe-Brown', 2069: 'Raff', 2070: 'Rahman', 2071: 'Ramirez', 2072: 'Ramos', 2073: 'Ramsbotham', 2074: 'Ramsden', 2075: 'Ramsey', 2076: 'Randall', 2077: 'Randell', 2078: 'Randolph', 2079: 'Rapuluolisa', 2080: 'Rapuokwu', 2081: 'Ratten', 2082: 'Rawling', 2083: 'Rawlings', 2084: 'Raymond', 2085: 'Raynor', 2086: 'Read', 2087: 'Reagan', 2088: 'Real', 2089: 'Rearick', 2090: 'Rechner', 2091: 'Redding', 2092: 'Reed', 2093: 'Rees', 2094: 'Reeves', 2095: 'Reichard', 2096: 'Reid', 2097: 'Reilly', 2098: 'Remington', 2099: 'Rendall', 2100: 'Rene', 2101: 'Renwick', 2102: 'Repina', 2103: 'Reppert', 2104: 'Retana', 2105: 'Revell', 2106: 'Reye', 2107: 'Reyes', 2108: 'Reyna', 2109: 'Reynolds', 2110: 'Rhodes', 2111: 'Ricci', 2112: 'Rice', 2113: 'Richards', 2114: 'Richardson', 2115: 'Rickard', 2116: 'Riddle', 2117: 'Ridley', 2118: 'Rieke', 2119: 'Riggs', 2120: 'Riley', 2121: 'Ringrose', 2122: 'Rios', 2123: 'Rioux', 2124: 'Rippey', 2125: 'Rischbieth', 2126: 'Rishel', 2127: 'Rita', 2128: 'Ritchie', 2129: 'Rivas', 2130: 'Rivera', 2131: 'Rivero', 2132: 'Rivers', 2133: 'Rizzo', 2134: 'Robb', 2135: 'Robe', 2136: 'Roberts', 2137: 'Robertson', 2138: 'Robinson', 2139: 'Robson', 2140: 'Rocher', 2141: 'Rogers', 2142: 'Rogova', 2143: 'Rohu', 2144: 'Rolon', 2145: 'Romani', 2146: 'Romano', 2147: 'Romeo', 2148: 'Romero', 2149: 'Romilly', 2150: 'Rooke', 2151: 'Root', 2152: 'Rose', 2153: 'Ross', 2154: 'Ross-Watt', 2155: 'Rossi', 2156: 'Rounsevell', 2157: 'Rouse', 2158: 'Rowe', 2159: 'Rowland', 2160: 'Rowley', 2161: 'Rowntree', 2162: 'Royster', 2163: 'Rozhkova', 2164: 'Rozier', 2165: 'Rubensohn', 2166: 'Rubeo', 2167: 'Rubin', 2168: 'Rudd', 2169: 'Rudduck', 2170: 'Rueda', 2171: 'Ruggiero', 2172: 'Ruiz', 2173: 'Runyon', 2174: 'Ruse', 2175: 'Russell', 2176: 'Russo', 2177: 'Ruth', 2178: 'Rutherford', 2179: 'Rutledge', 2180: 'Ryan', 2181: 'Ryrie', 2182: 'Saad', 2183: 'Sabbatini', 2184: 'Sacco', 2185: 'Sadler', 2186: 'Sadlier', 2187: 'Sagese', 2188: 'Sal', 2189: 'Salas', 2190: 'Salier', 2191: 'Salinas', 2192: 'Salmond', 2193: 'Salter', 2194: 'Samaniego', 2195: 'Samoylova', 2196: 'Samson', 2197: 'Samsonova', 2198: 'Samuel', 2199: 'Sanchez', 2200: 'Sandefur', 2201: 'Sanders', 2202: 'Sanderson', 2203: 'Sandover', 2204: 'Sanford', 2205: 'Sani', 2206: 'Santana', 2207: 'Santiago', 2208: 'Sargent', 2209: 'Sargood', 2210: 'Sarratt', 2211: 'Saunders', 2212: 'Sauve', 2213: 'Savage', 2214: 'Sawtell', 2215: 'Sazonova', 2216: 'Scannell', 2217: 'Schatz', 2218: 'Schiavone', 2219: 'Schmidt', 2220: 'Schnaars', 2221: 'Schneider', 2222: 'Schoenheimer', 2223: 'Schofield', 2224: 'Schroeder', 2225: 'Schwartz', 2226: 'Scott', 2227: 'Seabrook', 2228: 'Seccombe', 2229: 'Secombe', 2230: 'See', 2231: 'Seleznev', 2232: 'Selezneva', 2233: 'Seleznyov', 2234: 'Seleznyova', 2235: 'Sells', 2236: 'Selwyn', 2237: 'Semmens', 2238: 'Senior', 2239: 'Seppelt', 2240: 'Septimus', 2241: 'Serena', 2242: 'Sergeyev', 2243: 'Sergeyeva', 2244: 'Serra', 2245: 'Serrano', 2246: 'Severson', 2247: 'Shaffer', 2248: 'Shah', 2249: 'Shahan', 2250: 'Shand', 2251: 'Shao', 2252: 'Sharp', 2253: 'Sharpe', 2254: 'Sharwood', 2255: 'Shaver', 2256: 'Shaw', 2257: 'Shcherbakov', 2258: 'She', 2259: 'Shearston', 2260: 'Sheehan', 2261: 'Sheets', 2262: 'Sheffield', 2263: 'Shelby', 2264: 'Shelton', 2265: 'Shen', 2266: 'Shephard', 2267: 'Shepherd', 2268: 'Sheppard', 2269: 'Sherman', 2270: 'Shih', 2271: 'Shillito', 2272: 'Shipp', 2273: 'Shipton', 2274: 'Shoebridge', 2275: 'Sholes', 2276: 'Shoobridge', 2277: 'Short', 2278: 'Shubin', 2279: 'Shubina', 2280: 'Siciliani', 2281: 'Siciliano', 2282: 'Sidorov', 2283: 'Sidorova', 2284: 'Sievier', 2285: 'Silva', 2286: 'Simmons', 2287: 'Simon', 2288: 'Simpson', 2289: 'Sims', 2290: 'Sinclair', 2291: 'Singh', 2292: 'Singleton', 2293: 'Sinnett', 2294: 'Skelton', 2295: 'Skinner', 2296: 'Slate', 2297: 'Slater', 2298: 'Slattery', 2299: 'Sleeman', 2300: 'Slone', 2301: 'Small', 2302: 'Smalley', 2303: 'Smeaton', 2304: 'Smith', 2305: 'Sneddon', 2306: 'Snider', 2307: 'Sochima', 2308: 'Sokolov', 2309: 'Sokolova', 2310: 'Solomina', 2311: 'Solomon', 2312: 'Somadina', 2313: 'Somayina', 2314: 'Somerville', 2315: 'Sopuluchi', 2316: 'Sopuluchukwu', 2317: 'Sorenson', 2318: 'Sorokina', 2319: 'Soto', 2320: 'Soubeiran', 2321: 'Sousa', 2322: 'Souter', 2323: 'Sozonov', 2324: 'Sparks', 2325: 'Spaull', 2326: 'Spears', 2327: 'Spence', 2328: 'Spencer', 2329: 'Speth', 2330: 'Spinelli', 2331: 'Spitzer', 2332: 'Spring', 2333: 'Spyer', 2334: 'St Clair', 2335: 'Standish', 2336: 'Stange', 2337: 'Stanley', 2338: 'Stanton', 2339: 'Steele', 2340: 'Steen', 2341: 'Steere', 2342: 'Stehle', 2343: 'Steigrad', 2344: 'Steiner', 2345: 'Steinhoff', 2346: 'Stelzer', 2347: 'Stephens', 2348: 'Stephenson', 2349: 'Stetson', 2350: 'Stevens', 2351: 'Stevenson', 2352: 'Stewart', 2353: 'Stiger', 2354: 'Stiles', 2355: 'Stobie', 2356: 'Stokes', 2357: 'Stone', 2358: 'Stonebraker', 2359: 'Stoneman', 2360: 'Storey', 2361: 'Stout', 2362: 'Stradford', 2363: 'Streeten', 2364: 'Streeter', 2365: 'Streeton', 2366: 'Sturdee', 2367: 'Sturt', 2368: 'Su', 2369: 'Suffolk', 2370: 'Sugden', 2371: 'Sukhorukova', 2372: 'Sullivan', 2373: 'Summers', 2374: 'Summerville', 2375: 'Sun', 2376: 'Sunderland', 2377: 'Sung', 2378: 'Sutherland', 2379: 'Sutton', 2380: 'Suttor', 2381: 'Swaim', 2382: 'Swain', 2383: 'Swanson', 2384: 'Swanton', 2385: 'Swayne', 2386: 'Swearingen', 2387: 'Swift', 2388: 'Swinton', 2389: 'Sykes', 2390: 'Synnot', 2391: 'Szabados', 2392: \"T'an\", 2393: \"T'ang\", 2394: \"T'ao\", 2395: \"T'ien\", 2396: 'Tai', 2397: 'Takasuka', 2398: 'Talbot', 2399: 'Tan', 2400: 'Tang', 2401: 'Tao', 2402: 'Taplin', 2403: 'Tardent', 2404: 'Tate', 2405: 'Taubman', 2406: 'Taverner', 2407: 'Taylor', 2408: 'Teague', 2409: 'Temple', 2410: 'Templeman', 2411: 'Teng', 2412: 'Tennant', 2413: 'Terry', 2414: 'Thao', 2415: 'Thomas', 2416: 'Thompson', 2417: 'Thomsen', 2418: 'Thomson', 2419: 'Thornton', 2420: 'Thorpe', 2421: 'Threatt', 2422: 'Thurgood', 2423: 'Thynne', 2424: 'Tien', 2425: 'Tikhonov', 2426: 'Tilley', 2427: 'Tillman', 2428: 'Timms', 2429: 'Timperley', 2430: 'Ting', 2431: 'Tinline', 2432: 'Tipton', 2433: 'Tisdall', 2434: 'Titheradge', 2435: 'Titus', 2436: 'To Rot', 2437: 'Tobenna', 2438: 'Tobeolisa', 2439: 'Tochukwu', 2440: 'Todd', 2441: 'Tokareva', 2442: 'Tokaryev', 2443: 'Tomlinson', 2444: 'Toomey', 2445: 'Toosey', 2446: 'Torkelson', 2447: 'Torode', 2448: 'Torreggiani', 2449: 'Torres', 2450: 'Toscani', 2451: 'Toscano', 2452: 'Toth', 2453: 'Townsend', 2454: 'Townsley', 2455: 'Traeger', 2456: 'Tran', 2457: 'Travis', 2458: 'Treacy', 2459: 'Trejo', 2460: 'Trentini', 2461: 'Trentino', 2462: 'Tretiakov', 2463: 'Tretiakova', 2464: 'Tretyakova', 2465: 'Trevascus', 2466: 'Trevisan', 2467: 'Trevisani', 2468: 'Trevisano', 2469: 'Trouette', 2470: 'Troupe', 2471: 'Trout', 2472: 'Trujillo', 2473: 'Trumbull', 2474: 'Truscott', 2475: \"Ts'ai\", 2476: \"Ts'ao\", 2477: \"Ts'ui\", 2478: 'Tsai', 2479: 'Tsao', 2480: 'Tseng', 2481: 'Tsou', 2482: 'Tsui', 2483: 'Tu', 2484: 'Tuan', 2485: 'Tucker', 2486: 'Tudawali', 2487: 'Tung', 2488: 'Turnbull', 2489: 'Tychonoff', 2490: 'Tyler', 2491: 'Tyndall', 2492: 'Ubanwa', 2493: 'Uchechukwu', 2494: 'Uchenna', 2495: 'Udegbulam', 2496: 'Udegbunam', 2497: 'Udinese', 2498: 'Udinesi', 2499: 'Udobata', 2500: 'Udokamma', 2501: 'Ugochukwu', 2502: 'Ugochukwutubelum', 2503: 'Ugoji', 2504: 'Ugonna', 2505: 'Ugonnatubelum', 2506: 'Ugorji', 2507: 'Ukaegbulam', 2508: 'Ukaegbunam', 2509: 'Ulyanov', 2510: 'Ulyanova', 2511: 'Unaipon', 2512: 'Unwin', 2513: 'Upchurch', 2514: 'Upjohn', 2515: 'Uren', 2516: 'Uspenskaya', 2517: 'Uspensky', 2518: 'Utz', 2519: 'Uvarov', 2520: 'Uvarova', 2521: 'Uwaezuoke', 2522: 'Uwakwe', 2523: 'Vachon', 2524: 'Vagin', 2525: 'Vaguine', 2526: 'Valdez', 2527: 'Valenzuela', 2528: 'Vanmeter', 2529: 'Vanzetti', 2530: 'Vasiliev', 2531: 'Vasilieva', 2532: 'Vasilyev', 2533: 'Vasilyeva', 2534: 'Vasin', 2535: 'Vassiliev', 2536: 'Vavilov', 2537: 'Veale', 2538: 'Velazquez', 2539: 'Veltri', 2540: 'Venables', 2541: 'Verco', 2542: 'Verjus', 2543: 'Vessels', 2544: 'Vial', 2545: 'Vicars', 2546: 'Victor', 2547: 'Vida', 2548: 'Vidal', 2549: 'Vidler', 2550: 'Vincent', 2551: 'Vinogradoff', 2552: 'Vinogradov', 2553: 'Vinogradova', 2554: 'Virgo', 2555: 'Vogel', 2556: 'Volkov', 2557: 'Volkova', 2558: 'Von Doussa', 2559: 'Vorobyova', 2560: 'Voronina', 2561: 'Voronkov', 2562: 'Voronoff', 2563: 'Voss', 2564: 'Wade', 2565: 'Wagner', 2566: 'Wakelin', 2567: 'Walker', 2568: 'Walkom', 2569: 'Wall', 2570: 'Wallace', 2571: 'Wallis', 2572: 'Wallwork', 2573: 'Walpole', 2574: 'Walsh', 2575: 'Walters', 2576: 'Walton', 2577: 'Wan', 2578: 'Wang', 2579: 'Wanliss', 2580: 'Ward', 2581: 'Wardell', 2582: 'Wardle', 2583: 'Waring', 2584: 'Wark', 2585: 'Warlow-Davies', 2586: 'Warner', 2587: 'Warren', 2588: 'Waterhouse', 2589: 'Waters', 2590: 'Watkins', 2591: 'Watson', 2592: 'Watt', 2593: 'Watterston', 2594: 'Watts', 2595: 'Wearing', 2596: 'Weatherford', 2597: 'Weaver', 2598: 'Webb', 2599: 'Weber', 2600: 'Webster', 2601: 'Wei', 2602: 'Weigel', 2603: 'Welch', 2604: 'Weller', 2605: 'Welsh', 2606: 'Wentworth-Shields', 2607: 'Wenz', 2608: 'Wertheim', 2609: 'West', 2610: 'Westerberg', 2611: 'Weston', 2612: 'Wetherspoon', 2613: 'Wheare', 2614: 'Wheeler', 2615: 'Whiddon', 2616: 'White', 2617: 'Whitehead', 2618: 'Whitehouse', 2619: 'Whitelegge', 2620: 'Whitfield', 2621: 'Whitson', 2622: 'Whittaker', 2623: 'Whitworth', 2624: 'Wickens', 2625: 'Wieck', 2626: 'Wilder', 2627: 'Wilding', 2628: 'Wildman', 2629: 'Wiley', 2630: 'Wilhelm', 2631: 'Wilkes', 2632: 'Wilkins', 2633: 'Wilkinson', 2634: 'William', 2635: 'Williams', 2636: 'Williamson', 2637: 'Williford', 2638: 'Willis', 2639: 'Willmore', 2640: 'Willoughby', 2641: 'Wilsmore', 2642: 'Wilson', 2643: 'Wimble', 2644: 'Windradyne', 2645: 'Windsor', 2646: 'Winifred', 2647: 'Winn', 2648: 'Winter', 2649: 'Winter-Irving', 2650: 'Winters', 2651: 'Wisdom', 2652: 'Witt', 2653: 'Wolfe', 2654: 'Wollstonecraft', 2655: 'Wong', 2656: 'Wood', 2657: 'Woodard', 2658: 'Woodhouse', 2659: 'Woods', 2660: 'Woodward', 2661: 'Woolnough', 2662: 'Woronoff', 2663: 'Worsnop', 2664: 'Wreford', 2665: 'Wright', 2666: 'Wu', 2667: 'Wunder', 2668: 'Wyatt', 2669: 'Wyckoff', 2670: 'Wynn', 2671: 'Wynne', 2672: 'Y?', 2673: 'Y?an', 2674: 'Yamamoto', 2675: 'Yancy', 2676: 'Yang', 2677: 'Yao', 2678: 'Yashina', 2679: 'Yates', 2680: 'Yeates', 2681: 'Yefimov', 2682: 'Yefimova', 2683: 'Yefremov', 2684: 'Yefremova', 2685: 'Yegorov', 2686: 'Yegorova', 2687: 'Yeh', 2688: 'Yelverton', 2689: 'Yen', 2690: 'Yermakov', 2691: 'Yermakova', 2692: 'Yermolayev', 2693: 'Yermolayeva', 2694: 'Yevdokimova', 2695: 'Yevseyev', 2696: 'Yewen', 2697: 'Yin', 2698: 'Yip', 2699: 'Yirawala', 2700: 'Yobachi', 2701: 'Yobachukwu', 2702: 'Yobanna', 2703: 'Yocum', 2704: 'Yoo', 2705: 'Yost', 2706: 'Young', 2707: 'Younger', 2708: 'Yu', 2709: 'Yuan', 2710: 'Yudin', 2711: 'Yudina', 2712: 'Yuille', 2713: 'Yuriev', 2714: 'Yuryeva', 2715: 'Yusupov', 2716: 'Yusupova', 2717: 'Zack', 2718: 'Zaitsev', 2719: 'Zakharov', 2720: 'Zarate', 2721: 'Zaytseva', 2722: 'Zetticci', 2723: 'Zhdanov', 2724: 'Zhdanova', 2725: 'Zhirov', 2726: 'Zhou', 2727: 'Zikoranachidimma', 2728: 'Zikoranachukwudimma', 2729: 'Zikoranaudodimma', 2730: 'Zimmer', 2731: 'Zinachukwudi', 2732: 'Zito', 2733: 'Zotova', 2734: 'Zox', 2735: 'Zubarev', 2736: 'Zuev', 2737: 'Zuyev', 2738: 'Zuyeva'}\n",
            "Distribusi kolom numerik Surname:\n",
            "Surname\n",
            "1142    1951\n",
            "2395    1853\n",
            "1140    1304\n",
            "1525    1268\n",
            "1267    1262\n",
            "        ... \n",
            "656        1\n",
            "1449       1\n",
            "2527       1\n",
            "1669       1\n",
            "1774       1\n",
            "Name: count, Length: 2739, dtype: int64 \n",
            "\n",
            "Nilai minimum untuk kolom numerik Surname:\n",
            "0 \n",
            "\n",
            "Nilai maksimum untuk kolom numerik Surname:\n",
            "2738 \n",
            "\n",
            "Mapping untuk kolom  non numerikGeography: {0: 'France', 1: 'Germany', 2: 'Spain'}\n",
            "Distribusi kolom numerik Geography:\n",
            "Geography\n",
            "0    75465\n",
            "2    28869\n",
            "1    27666\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Nilai minimum untuk kolom numerik Geography:\n",
            "0 \n",
            "\n",
            "Nilai maksimum untuk kolom numerik Geography:\n",
            "2 \n",
            "\n",
            "Mapping untuk kolom  non numerikGender: {0: 'Female', 1: 'Male'}\n",
            "Distribusi kolom numerik Gender:\n",
            "Gender\n",
            "1    74357\n",
            "0    57643\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Nilai minimum untuk kolom numerik Gender:\n",
            "0 \n",
            "\n",
            "Nilai maksimum untuk kolom numerik Gender:\n",
            "1 \n",
            "\n",
            "Mapping untuk kolom  non numerikSurname: {0: 'Abazu', 1: 'Abbie', 2: 'Abbott', 3: 'Abdulov', 4: 'Abernathy', 5: 'Abramova', 6: 'Abramovich', 7: 'Abramowitz', 8: 'Abrego', 9: 'Achebe', 10: 'Adams', 11: 'Adamson', 12: 'Afamefula', 13: 'Afamefuna', 14: 'Afanasyev', 15: 'Afanasyeva', 16: 'Agafonova', 17: 'Aguirre', 18: 'Ah Mouy', 19: 'Ahern', 20: 'Ahmed', 21: 'Aiken', 22: 'Aikenhead', 23: 'Ainsworth', 24: 'Aitken', 25: 'Ajuluchukwu', 26: 'Akabueze', 27: 'Akeroyd', 28: 'Akhtar', 29: 'Akobundu', 30: 'Aksakova', 31: 'Aksenov', 32: 'Aksenova', 33: 'Aksyonov', 34: 'Aksyonova', 35: 'Akubundu', 36: 'Akudinobi', 37: 'Alaniz', 38: 'Alderete', 39: 'Aldrich', 40: 'Aleksandrova', 41: 'Alekseeva', 42: 'Alekseyeva', 43: 'Aleshire', 44: 'Alexander', 45: 'Alexandrova', 46: 'Alexeeva', 47: 'Alexeyeva', 48: 'Algeranoff', 49: 'Ali', 50: 'Aliyev', 51: 'Aliyeva', 52: 'Allan', 53: 'Allardyce', 54: 'Allen', 55: 'Alley', 56: 'Alleyne', 57: 'Allingham', 58: 'Allsop', 59: 'Alvarez', 60: 'Amadi', 61: 'Amaechi', 62: 'Amechi', 63: 'Ampt', 64: 'Anayochukwu', 65: 'Anayolisa', 66: 'Andersen', 67: 'Anderson', 68: 'Andreev', 69: 'Andrejew', 70: 'Andrews', 71: 'Andreyev', 72: 'Andreyeva', 73: 'Anenechi', 74: 'Anenechukwu', 75: 'Angel', 76: 'Angelo', 77: 'Ankudinov', 78: 'Ansell', 79: 'Aparicio', 80: 'Arbour', 81: 'Archer', 82: 'Arcuri', 83: 'Argyle', 84: 'Armfield', 85: 'Arnold', 86: 'Arrington', 87: 'Artemiev', 88: 'Arthur', 89: 'Artyomova', 90: 'Ash', 91: 'Ashbolt', 92: 'Ashton', 93: 'Atherton', 94: 'Atkins', 95: 'Atkinson', 96: 'Austin', 97: 'Avdeev', 98: 'Avdeeva', 99: 'Avent', 100: 'Ayers', 101: 'Azarov', 102: 'Azikiwe', 103: 'Azubuike', 104: 'Azuka', 105: 'Babbage', 106: 'Baddeley', 107: 'Bage', 108: 'Bailey', 109: 'Bair', 110: 'Baker', 111: 'Balashov', 112: 'Balashova', 113: 'Bales', 114: 'Ball', 115: 'Ballard', 116: 'Balmain', 117: 'Balsillie', 118: 'Bancks', 119: 'Bancroft', 120: 'Band', 121: 'Banks', 122: 'Baranov', 123: 'Baranova', 124: 'Barber', 125: 'Barbour', 126: 'Barclay-Harvey', 127: 'Barese', 128: 'Baresi', 129: 'Barker', 130: 'Barling', 131: 'Barlow', 132: 'Barnard', 133: 'Barnes', 134: 'Barnet', 135: 'Barnett', 136: 'Barrera', 137: 'Barrett', 138: 'Barry', 139: 'Bartlett', 140: 'Barton', 141: 'Baryshnikov', 142: 'Bates', 143: 'Bateson', 144: 'Batty', 145: 'Baxter', 146: 'Bazarova', 147: 'Bazhenov', 148: 'Beale', 149: 'Beatham', 150: 'Beavers', 151: 'Becher', 152: 'Beck', 153: 'Becker', 154: 'Bednall', 155: 'Beede', 156: 'Beers', 157: 'Beggs', 158: 'Begum', 159: 'Belisario', 160: 'Bell', 161: 'Bellasis', 162: 'Bellucci', 163: 'Belonwu', 164: 'Belousov', 165: 'Belstead', 166: 'Beluchi', 167: 'Beneventi', 168: 'Benford', 169: 'Benjamin', 170: 'Bennelong', 171: 'Bennett', 172: 'Bennetts', 173: 'Benson', 174: 'Bentley', 175: 'Bergamaschi', 176: 'Berry', 177: 'Bess', 178: 'Bevan', 179: 'Bevington', 180: 'Bezrukov', 181: 'Bezrukova', 182: 'Bianchi', 183: 'Bibi', 184: 'Billson', 185: 'Binder', 186: 'Bird', 187: 'Birdsall', 188: 'Birdseye', 189: 'Birk', 190: 'Biryukov', 191: 'Biryukova', 192: 'Bischof', 193: 'Bishop', 194: 'Bitter', 195: 'Black', 196: 'Blackburn', 197: 'Blacklock', 198: 'Blackwood', 199: 'Blake', 200: 'Bledsoe', 201: 'Blesing', 202: 'Bligh', 203: 'Blinova', 204: 'Blue', 205: 'Boag', 206: 'Boan', 207: 'Board', 208: 'Bobrov', 209: 'Bocharova', 210: 'Bochsa', 211: 'Bock', 212: 'Bogdanov', 213: 'Bogdanova', 214: 'Bogle', 215: 'Bogolyubov', 216: 'Bogolyubova', 217: 'Bolton', 218: 'Boni', 219: 'Boniwell', 220: 'Booth', 221: 'Bottrill', 222: 'Boulger', 223: 'Bovee', 224: 'Bowen', 225: 'Bowhay', 226: 'Boyd', 227: 'Boylan', 228: 'Boyle', 229: 'Bracewell', 230: 'Bradley', 231: 'Bradshaw', 232: 'Brady', 233: 'Bray', 234: 'Brazenor', 235: 'Bremer', 236: 'Brenan', 237: 'Brennan', 238: 'Brient', 239: 'Brierly', 240: 'Briggs', 241: 'Brigstocke', 242: 'Broadhurst', 243: 'Brock', 244: 'Bromby', 245: 'Bromley', 246: 'Brookes', 247: 'Brooks', 248: 'Brothers', 249: 'Brown', 250: 'Browne', 251: 'Brownless', 252: 'Brownlow', 253: 'Bruce', 254: 'Bruno', 255: 'Bruny', 256: 'Bryant', 257: 'Buccho', 258: 'Buchanan', 259: 'Buchi', 260: 'Buckland', 261: 'Buckley', 262: 'Buckner', 263: 'Buddicom', 264: 'Bufkin', 265: 'Builder', 266: 'Bukowski', 267: 'Buley', 268: 'Bulgakov', 269: 'Bull', 270: 'Burbidge', 271: 'Burdekin', 272: 'Burgess', 273: 'Burgmann', 274: 'Burgos', 275: 'Burgoyne', 276: 'Burke', 277: 'Burlingame', 278: 'Burn', 279: 'Burns', 280: 'Burt', 281: 'Burtch', 282: 'Burton', 283: 'Butcher', 284: 'Butler', 285: 'Buttenshaw', 286: 'Butusov', 287: 'Byrne', 288: 'Caffyn', 289: 'Cairns', 290: 'Calabrese', 291: 'Calabresi', 292: 'Caldwell', 293: 'Calzada', 294: 'Cambage', 295: 'Cameron', 296: 'Campbell', 297: 'Cantamessa', 298: 'Cantrell', 299: 'Capon', 300: 'Cardell', 301: 'Carey', 302: 'Carlson', 303: 'Carpenter', 304: 'Carr', 305: 'Carroll', 306: 'Carruthers', 307: 'Carslaw', 308: 'Carter', 309: 'Cartwright', 310: 'Carvosso', 311: 'Cary', 312: 'Cashin', 313: 'Castiglione', 314: 'Castles', 315: 'Cattaneo', 316: 'Cavenagh', 317: 'Cavill', 318: 'Cawker', 319: 'Cawood', 320: 'Cawthorne', 321: 'Cayley', 322: 'Celis', 323: \"Ch'ang\", 324: \"Ch'en\", 325: \"Ch'eng\", 326: \"Ch'ien\", 327: \"Ch'in\", 328: \"Ch'iu\", 329: 'Challis', 330: 'Chamberlain', 331: 'Chamberlin', 332: 'Chambers', 333: 'Champion', 334: 'Chan', 335: 'Chang', 336: 'Chao', 337: 'Charlton', 338: 'Cheatham', 339: 'Chen', 340: 'Cheng', 341: 'Cherkasova', 342: 'Chia', 343: 'Chiabuotu', 344: 'Chiagoziem', 345: 'Chialuka', 346: 'Chiang', 347: 'Chiawuotu', 348: 'Chiazagomekpele', 349: 'Chiazagomekpere', 350: 'Chibueze', 351: 'Chibugo', 352: 'Chibuzo', 353: 'Chidalu', 354: 'Chidi', 355: 'Chidiebele', 356: 'Chidiebere', 357: 'Chidiegwu', 358: 'Chidimma', 359: 'Chidozie', 360: 'Chidubem', 361: 'Chidumaga', 362: 'Chiebuka', 363: 'Chiedozie', 364: 'Chiefo', 365: 'Chiekwugo', 366: 'Chieloka', 367: 'Chiemeka', 368: 'Chiemela', 369: 'Chiemenam', 370: 'Chiemezie', 371: 'Chien', 372: 'Chienezie', 373: 'Chifley', 374: 'Chifo', 375: 'Chiganu', 376: 'Chigbogu', 377: 'Chigolum', 378: 'Chigozie', 379: 'Chijindum', 380: 'Chijioke', 381: 'Chikelu', 382: 'Chikere', 383: 'Chikezie', 384: 'Chikwado', 385: 'Chikwendu', 386: 'Chimaijem', 387: 'Chimaobim', 388: 'Chimaoke', 389: 'Chimezie', 390: 'Chin', 391: 'Chinagorom', 392: 'Chinedum', 393: 'Chineze', 394: 'Chinomso', 395: 'Chinonyelum', 396: 'Chinweike', 397: 'Chinwemma', 398: 'Chinwendu', 399: 'Chinwenma', 400: 'Chinweuba', 401: 'Chioke', 402: 'Chiu', 403: 'Chiwetelu', 404: 'Chizoba', 405: 'Chizuoke', 406: 'Chong', 407: 'Chou', 408: 'Christie', 409: 'Christmas', 410: 'Chu', 411: 'Chuang', 412: 'Chubb', 413: 'Chukwualuka', 414: 'Chukwubuikem', 415: 'Chukwudi', 416: 'Chukwuebuka', 417: 'Chukwueloka', 418: 'Chukwuemeka', 419: 'Chukwufumnanya', 420: 'Chukwuhaenye', 421: 'Chukwujamuike', 422: 'Chukwujekwu', 423: 'Chukwukadibia', 424: 'Chukwukere', 425: 'Chukwuma', 426: 'Chukwumaobim', 427: 'Chukwunonso', 428: 'Chukwuraenye', 429: 'Chung', 430: 'Chuter', 431: 'Cisneros', 432: 'Claiborne', 433: 'Clamp', 434: 'Clapp', 435: 'Clark', 436: 'Clarke', 437: 'Clayton', 438: 'Clements', 439: 'Coates', 440: 'Cobb', 441: 'Cocci', 442: 'Cockrum', 443: 'Cody', 444: 'Coffey', 445: 'Cohn', 446: 'Coles', 447: 'Collee', 448: 'Collier', 449: 'Collins', 450: 'Colombo', 451: 'Combs', 452: 'Cone', 453: 'Congreve', 454: 'Connely', 455: 'Connolly', 456: 'Connor', 457: 'Conti', 458: 'Converse', 459: 'Conway', 460: 'Cook', 461: 'Cooper', 462: 'Corby', 463: 'Corones', 464: 'Corran', 465: 'Corrie', 466: 'Corser', 467: 'Corson', 468: 'Costa', 469: 'Coupp', 470: 'Cousens', 471: 'Cowger', 472: 'Cox', 473: 'Craig', 474: 'Crawford', 475: 'Cremonesi', 476: 'Creswell', 477: 'Cribb', 478: 'Crist', 479: 'Crocker', 480: 'Cross', 481: 'Crotty', 482: 'Crumbley', 483: 'Crump', 484: 'Cumbrae-Stewart', 485: 'Cummins', 486: 'Cunningham', 487: 'Curnow', 488: 'Curtis', 489: 'Cyril', 490: 'Czajkowski', 491: \"D'Albertis\", 492: 'Dahlenburg', 493: 'Dale', 494: 'Dalrymple', 495: 'Dalton', 496: 'Daluchi', 497: 'Daly', 498: 'Dancy', 499: 'Daniels', 500: 'Dann', 501: 'Darling', 502: 'Darwin', 503: 'Davey', 504: 'Davide', 505: 'Davidson', 506: 'Davies', 507: 'Davila', 508: 'Davis', 509: 'Davison', 510: 'Davy', 511: 'Davydova', 512: 'Dawson', 513: 'Day', 514: 'De Bernales', 515: 'De Garis', 516: 'De Luca', 517: 'De Mestre', 518: 'De Neeve', 519: 'De Salis', 520: 'DeRose', 521: 'Dean', 522: 'Debellis', 523: 'Defalco', 524: 'Degtyaryov', 525: 'Dellucci', 526: 'Denisov', 527: 'Denisova', 528: 'Dennis', 529: 'Dennys', 530: 'Descoteaux', 531: 'Despeissis', 532: 'Dettmann', 533: 'Diaz', 534: 'Dickinson', 535: 'Dickson', 536: 'Dietz', 537: 'Diggs', 538: 'Dike', 539: 'Dilibe', 540: 'Dilke', 541: 'Dillon', 542: 'Diribe', 543: 'Dobbs', 544: 'Docherty', 545: 'Dodd', 546: 'Dodds', 547: 'Dodgshun', 548: 'Doherty', 549: 'Dolgorukova', 550: 'Dominguez', 551: 'Donahue', 552: 'Donaldson', 553: 'Donnelly', 554: 'Dore', 555: 'Douglas', 556: 'Downie', 557: 'Dowse', 558: 'Doyle', 559: 'Doyne', 560: 'Dreyer', 561: 'Drury', 562: 'Dubinina', 563: 'Dufresne', 564: 'Duigan', 565: 'Duke', 566: 'Dulhunty', 567: 'Dumetochukwu', 568: 'Dunbabin', 569: 'Duncan', 570: 'Dunn', 571: 'Dyer', 572: 'Dyson', 573: 'Eames', 574: 'Earl', 575: 'Ebelechukwu', 576: 'Ebelegbulam', 577: 'Eberegbulam', 578: 'Echezonachukwu', 579: 'Edith', 580: 'Edments', 581: 'Edmondstone', 582: 'Edwards', 583: 'Efimov', 584: 'Efremov', 585: 'Efremova', 586: 'Egobudike', 587: 'Eiland', 588: 'Eipper', 589: 'Ejikemeifeuwa', 590: 'Ejimofor', 591: 'Ekechukwu', 592: 'Elewechi', 593: 'Elkins', 594: 'Elliot', 595: 'Elliott', 596: 'Ellis', 597: 'Elmore', 598: 'Eluemuno', 599: 'Emenike', 600: 'Emery', 601: 'Endrizzi', 602: 'Enemuo', 603: 'Enyinnaya', 604: 'Eremenko', 605: 'Ermakov', 606: 'Ershova', 607: 'Erskine', 608: 'Esomchi', 609: 'Espinosa', 610: 'Esposito', 611: 'Estep', 612: 'Estrada', 613: 'Evans', 614: 'Evdokimov', 615: 'Evseev', 616: 'Evseyev', 617: 'Ewen', 618: 'Ewers', 619: 'Ewing', 620: 'Fadden', 621: 'Fallaci', 622: 'Fan', 623: 'Fancher', 624: 'Fane', 625: 'Fang', 626: 'Fantin', 627: 'Fanucci', 628: 'Farber', 629: 'Faria', 630: 'Farmer', 631: 'Farrar', 632: 'Farrell', 633: 'Faulkner', 634: 'Favors', 635: 'Fedorov', 636: 'Fedorova', 637: 'Feetham', 638: 'Felix', 639: 'Feng', 640: 'Fennell', 641: 'Fennescey', 642: 'Fenton', 643: 'Ferguson', 644: 'Fernando', 645: 'Ferrari', 646: 'Ferri', 647: 'Fetherstonhaugh', 648: 'Field', 649: 'Fielding', 650: 'Finch', 651: 'Findlay', 652: 'Fiore', 653: 'Fiorentini', 654: 'Fiorentino', 655: 'Fisher', 656: 'Fisk', 657: 'Fitch', 658: 'Fitts', 659: 'Fitzgerald', 660: 'Fleetwood-Smith', 661: 'Fleming', 662: 'Flemming', 663: 'Fletcher', 664: 'Floyd', 665: 'Flynn', 666: 'Fokina', 667: 'Fokine', 668: 'Folliero', 669: 'Fomin', 670: 'Fomina', 671: 'Fontaine', 672: 'Fontenot', 673: 'Forbes', 674: 'Ford', 675: 'Forster', 676: 'Forwood', 677: 'Foster', 678: 'Foveaux', 679: 'Fowler', 680: 'Fox', 681: 'Francis', 682: 'Franklin', 683: 'Fraser', 684: 'Frederickson', 685: 'Freeman', 686: 'French', 687: 'Frewin', 688: 'Fries', 689: 'Froggatt', 690: 'Frolov', 691: 'Frolova', 692: 'Frost', 693: 'Fu', 694: 'Fulks', 695: 'Fuller', 696: 'Fullwood', 697: 'Furneaux', 698: 'Fyans', 699: 'Fyodorov', 700: 'Fyodorova', 701: 'Gadsden', 702: 'Gadsdon', 703: 'Gaffney', 704: 'Galgano', 705: 'Galkin', 706: 'Galkina', 707: 'Gallagher', 708: 'Gallo', 709: 'Galloway', 710: 'Gambrell', 711: 'Gannon', 712: 'Gant', 713: 'Garcia', 714: 'Gardener', 715: 'Gardiner', 716: 'Gardner', 717: 'Garland', 718: 'Garmon', 719: 'Garner', 720: 'Garran', 721: 'Garrett', 722: 'Gartrell', 723: 'Gay', 724: 'Gboliwe', 725: 'Geach', 726: 'Gearheart', 727: 'Gebhart', 728: 'Gell', 729: 'Genovese', 730: 'Genovesi', 731: 'Gentry', 732: 'Geoghegan', 733: 'George', 734: 'Gerald', 735: 'Gerasimov', 736: 'Gerasimova', 737: 'Gether', 738: 'Gibbons', 739: 'Gibbs', 740: 'Gibson', 741: 'Gilbert', 742: 'Gilchrist', 743: 'Giles', 744: 'Gill', 745: 'Gilleland', 746: 'Gilroy', 747: 'Ginikanwa', 748: 'Ginn', 749: 'Giordano', 750: 'Glasgow', 751: 'Glassman', 752: 'Glennon', 753: 'Glossop', 754: 'Glover', 755: 'Goddard', 756: 'Godfrey', 757: 'Godson', 758: 'Goering', 759: 'Golibe', 760: 'Goliwe', 761: 'Goloubev', 762: 'Golovanov', 763: 'Golubev', 764: 'Golubeva', 765: 'Golubov', 766: 'Golubova', 767: 'Gomes', 768: 'Gonzalez', 769: 'Goodwin', 770: 'Gorbunov', 771: 'Gorbunova', 772: 'Gordon', 773: 'Gorman', 774: 'Gorshkov', 775: 'Gosnell', 776: 'Gouger', 777: 'Gough', 778: 'Gould', 779: 'Gow', 780: 'Graham', 781: 'Grant', 782: 'Gratton', 783: 'Grave', 784: 'Gray', 785: 'Greaves', 786: 'Greco', 787: 'Greece', 788: 'Green', 789: 'Greene', 790: 'Greenhalgh', 791: 'Greenwalt', 792: 'Gregory', 793: 'Gresswell', 794: 'Grieve', 795: 'Griffen', 796: 'Griffin', 797: 'Griffiths', 798: 'Grigoryeva', 799: 'Groom', 800: 'Grosse', 801: 'Grover', 802: 'Grubb', 803: 'Guerin', 804: 'Gunson', 805: 'H?', 806: 'Hackett', 807: 'Haddon', 808: 'Hagins', 809: 'Hale', 810: 'Hales', 811: 'Hall', 812: 'Hallahan', 813: 'Hamilton', 814: 'Hammonds', 815: 'Hampton', 816: 'Han', 817: 'Hancock', 818: 'Hand', 819: 'Hanna', 820: 'Hansen', 821: 'Hanson', 822: 'Hao', 823: 'Hardiman', 824: 'Harding', 825: 'Hardy', 826: 'Hare', 827: 'Harewood', 828: 'Hargrave', 829: 'Hargraves', 830: 'Hargreaves', 831: 'Harker', 832: 'Harper', 833: 'Harrell', 834: 'Harriman', 835: 'Harris', 836: 'Harrison', 837: 'Hart', 838: 'Hartung', 839: 'Harvey', 840: 'Hassall', 841: 'Haugh', 842: 'Hawes', 843: 'Hawkins', 844: 'Hawks', 845: 'Haworth', 846: 'Hawthorn', 847: 'Hay', 848: 'Hayden', 849: 'Hayes', 850: 'Hayslett', 851: 'Hayward', 852: 'Hazon', 853: 'He', 854: 'Heath', 855: 'Henderson', 856: 'Hendley', 857: 'Henry', 858: 'Henty', 859: 'Herbert', 860: 'Hernandez', 861: 'Herrera', 862: 'Herring', 863: 'Herz', 864: 'Hess', 865: 'Hewitt', 866: 'Heydon', 867: 'Hickey', 868: 'Hicks', 869: 'Higgins', 870: 'Highland', 871: 'Higinbotham', 872: 'Hill', 873: 'Hilton', 874: 'Hingston', 875: 'Hinton', 876: 'Hirst', 877: 'Hixson', 878: 'Ho', 879: 'Hobbs', 880: 'Hodge', 881: 'Hoelscher', 882: 'Holden', 883: 'Holder', 884: 'Hollis', 885: 'Holloway', 886: 'Holman', 887: 'Holmes', 888: 'Holt', 889: 'Hoolan', 890: 'Hooper', 891: 'Hope', 892: 'Hopetoun', 893: 'Hopkins', 894: 'Hopwood', 895: 'Horan', 896: 'Horsfall', 897: 'Horsley', 898: 'Horton', 899: 'Hou', 900: 'Houghton', 901: 'Hovell', 902: 'Howard', 903: 'Howarth', 904: 'Howe', 905: 'Howell', 906: 'Howells', 907: 'Hs?', 908: 'Hs?eh', 909: 'Hsia', 910: 'Hsiao', 911: 'Hsieh', 912: 'Hsing', 913: 'Hsiung', 914: 'Hsu', 915: 'Hsueh', 916: 'Hu', 917: 'Huang', 918: 'Huddart', 919: 'Hudson', 920: 'Huggins', 921: 'Hughes', 922: 'Hughes-Jones', 923: 'Humffray', 924: 'Humphreys', 925: 'Humphries', 926: 'Hung', 927: 'Hunt', 928: 'Hunter', 929: 'Hurst', 930: 'Hussain', 931: 'Hutchinson', 932: 'Hyde', 933: 'Hysell', 934: 'Iadanza', 935: 'Ibeabuchi', 936: 'Ibeamaka', 937: 'Ibekwe', 938: 'Ibezimako', 939: 'Ibragimova', 940: 'Ibrahimov', 941: 'Ibrahimova', 942: 'Ifeajuna', 943: 'Ifeanacho', 944: 'Ifeanyichukwu', 945: 'Ifeatu', 946: 'Ifesinachi', 947: 'Ignatieff', 948: 'Ignatiev', 949: 'Ignatyev', 950: 'Ignatyeva', 951: 'Iheanacho', 952: 'Iheatu', 953: 'Ijendu', 954: 'Ikechukwu', 955: 'Ikedinachukwu', 956: 'Ikemefuna', 957: 'Ikenna', 958: 'Illingworth', 959: 'Iloabuchi', 960: 'Iloerika', 961: 'Ilyina', 962: 'Ingamells', 963: 'Ingram', 964: 'Innes', 965: 'Iqbal', 966: 'Iredale', 967: 'Ireland', 968: 'Iroawuchi', 969: 'Isayev', 970: 'Isayeva', 971: 'Istomin', 972: 'Ives', 973: 'Iweobiegbulam', 974: 'Iweobiegbunam', 975: 'Jacka', 976: 'Jackson', 977: 'Jamieson', 978: 'Jamison', 979: 'Jarvis', 980: 'Jefferies', 981: 'Jen', 982: 'Jenkins', 983: 'Jenks', 984: 'Jennings', 985: 'Jensen', 986: 'Jess', 987: 'Jessop', 988: 'Jibunoh', 989: 'Jideofor', 990: 'Jobson', 991: 'John', 992: 'Johnson', 993: 'Johnston', 994: 'Johnstone', 995: 'Jolly', 996: 'Jonathan', 997: 'Jones', 998: 'Jordan', 999: 'Jose', 1000: 'Joslin', 1001: 'Jowett', 1002: 'Judd', 1003: \"K'ung\", 1004: 'K?', 1005: 'Kable', 1006: 'Kaeppel', 1007: 'Kaleski', 1008: 'Kalinina', 1009: 'Kambinachi', 1010: 'Kamdibe', 1011: 'Kanayochukwu', 1012: 'Kang', 1013: 'Kao', 1014: 'Kaodilinakachukwu', 1015: 'Kapustin', 1016: 'Kapustina', 1017: 'Kashiwagi', 1018: 'Kauffmann', 1019: 'Kaur', 1020: 'Kay', 1021: 'Kazakova', 1022: 'Kazantsev', 1023: 'Kazantseva', 1024: 'Keane', 1025: 'Keeley', 1026: 'Kegley', 1027: 'Kelechi', 1028: 'Kelley', 1029: 'Kelly', 1030: 'Kemp', 1031: 'Kenechi', 1032: 'Kenechukwu', 1033: 'Kenenna', 1034: 'Kennedy', 1035: 'Kenniff', 1036: 'Kent', 1037: 'Kepley', 1038: 'Kerr', 1039: 'Kershaw', 1040: 'Kesteven', 1041: 'Khan', 1042: 'Kharitonova', 1043: 'Kharlamov', 1044: 'Kharlamova', 1045: 'Kibble', 1046: 'Kibby', 1047: 'Kiernan', 1048: 'Kilgour', 1049: 'Kincaid', 1050: 'King', 1051: 'Kingsley', 1052: 'Kinlaw', 1053: 'Kirby', 1054: 'Kirillova', 1055: 'Kirk', 1056: 'Kirkland', 1057: 'Kirsova', 1058: 'Kisch', 1059: 'Kistler', 1060: 'Kline', 1061: 'Knepper', 1062: 'Knight', 1063: 'Knipe', 1064: 'Knorr', 1065: 'Knowles', 1066: 'Knox', 1067: 'Ko', 1068: 'Koch', 1069: 'Kodilinyechukwu', 1070: 'Koehler', 1071: 'Koger', 1072: 'Kolesnikov', 1073: 'Kolesnikova', 1074: 'Konovalova', 1075: 'Koo', 1076: 'Kornilova', 1077: 'Korovin', 1078: 'Korovina', 1079: 'Kosisochukwu', 1080: 'Kovalev', 1081: 'Kovaleva', 1082: 'Kovalyova', 1083: 'Kozlova', 1084: 'Kramer', 1085: 'Krawczyk', 1086: 'Krichauff', 1087: 'Kruglov', 1088: 'Kruglova', 1089: 'Krylov', 1090: 'Kryukov', 1091: 'Kryukova', 1092: 'Ku', 1093: 'Kudryashova', 1094: 'Kulikova', 1095: 'Kumm', 1096: 'Kung', 1097: 'Kuo', 1098: 'Kuykendall', 1099: 'Kuznetsova', 1100: 'Kwemto', 1101: 'Kwemtochukwu', 1102: 'L?', 1103: 'Lafleur', 1104: 'Lahti', 1105: 'Lai', 1106: 'Laidley', 1107: 'Lajoie', 1108: 'Lamb', 1109: 'Lambert', 1110: 'Lampungmeiua', 1111: 'Landman', 1112: 'Landor', 1113: 'Lane', 1114: 'Laney', 1115: 'Lanford', 1116: 'Lang', 1117: 'Langdon', 1118: 'Langler', 1119: 'Lappin', 1120: 'Larionova', 1121: 'Larkin', 1122: 'Lascelles', 1123: 'Lassetter', 1124: 'Lattimore', 1125: 'Laurie', 1126: 'Lavarack', 1127: 'Lavine', 1128: 'Lavrentiev', 1129: 'Lavrov', 1130: 'Law', 1131: 'Lawrence', 1132: 'Lawson', 1133: 'Lazar', 1134: 'Lazarev', 1135: 'Lazareva', 1136: 'Le Gallienne', 1137: 'Le Grand', 1138: 'Leach', 1139: 'Leak', 1140: 'Lear', 1141: 'Ledford', 1142: 'Lee', 1143: 'Lees', 1144: 'Lehr', 1145: 'Lei', 1146: 'Leibius', 1147: 'Leonard', 1148: 'Lettiere', 1149: 'Levan', 1150: 'Levi', 1151: 'Levien', 1152: 'Lewis', 1153: 'Leworthy', 1154: 'Li', 1155: 'Li Fonti', 1156: 'Liang', 1157: 'Liao', 1158: 'Lilly', 1159: 'Lin', 1160: 'Lindell', 1161: 'Lindsay', 1162: 'Linger', 1163: 'Linton', 1164: 'Lionel', 1165: 'Lipton', 1166: 'Lira', 1167: 'Liston', 1168: 'Little', 1169: 'Liu', 1170: 'Lloyd', 1171: 'Lo', 1172: 'Lo Duca', 1173: 'Loane', 1174: 'Lock', 1175: 'Locke', 1176: 'Lockington', 1177: 'Lockyer', 1178: 'Loewenthal', 1179: 'Loftus', 1180: 'Loggia', 1181: 'Loginov', 1182: 'Logue', 1183: 'Lombardi', 1184: 'Lombardo', 1185: 'Long', 1186: 'Longo', 1187: 'Longstaff', 1188: 'Lord', 1189: 'Lorenzo', 1190: 'Lori', 1191: 'Lorimer', 1192: 'Louis', 1193: 'Lowe', 1194: 'Lowell', 1195: 'Loyau', 1196: 'Lu', 1197: 'Lucas', 1198: 'Lucchese', 1199: 'Lucchesi', 1200: 'Lucciano', 1201: 'Lueck', 1202: 'Lujan', 1203: 'Lung', 1204: 'Lupton', 1205: 'Lynch', 1206: 'Lynton', 1207: 'Lysaght', 1208: 'Ma', 1209: 'MacDevitt', 1210: 'MacDonald', 1211: 'MacDonnell', 1212: 'MacPherson', 1213: 'Macadam', 1214: 'Macarthur', 1215: 'Macartney', 1216: 'Macdonald', 1217: 'Macfarlan', 1218: 'Macgroarty', 1219: 'Mach', 1220: 'Macintyre', 1221: 'Mackay', 1222: 'Mackenzie', 1223: 'Mackey', 1224: 'Macknight', 1225: 'Maclean', 1226: 'Macleod', 1227: 'Macnamara', 1228: 'Maconochie', 1229: 'Macrossan', 1230: 'Mactier', 1231: 'Macvitie', 1232: 'Madison', 1233: 'Madueke', 1234: 'Madukaego', 1235: 'Madukwe', 1236: 'Maggard', 1237: 'Mahmood', 1238: 'Mahon', 1239: 'Mai', 1240: 'Mairinger', 1241: 'Maitland', 1242: 'Malloy', 1243: 'Mamelu', 1244: 'Mancini', 1245: 'Manfrin', 1246: 'Mann', 1247: 'Manna', 1248: 'Manners', 1249: 'Manning', 1250: 'Mao', 1251: 'Marcelo', 1252: 'Marchesi', 1253: 'Marino', 1254: 'Mario', 1255: 'Marks', 1256: 'Marquez', 1257: 'Marsden', 1258: 'Marsh', 1259: 'Marshall', 1260: 'Marshall-Hall', 1261: 'Martel', 1262: 'Martin', 1263: 'Martinez', 1264: 'Mashman', 1265: 'Maslov', 1266: 'Maslova', 1267: 'Maslow', 1268: 'Mason', 1269: 'Massie', 1270: 'Matlock', 1271: 'Matthews', 1272: 'Matveyev', 1273: 'Matveyeva', 1274: 'Maughan', 1275: 'Mault', 1276: 'Maurer', 1277: 'May', 1278: 'Maynard', 1279: 'Mayrhofer', 1280: 'Mays', 1281: 'Mazure', 1282: 'Mazzanti', 1283: 'Mazzi', 1284: 'Mbadiwe', 1285: 'Mbanefo', 1286: 'McBurney', 1287: 'McCaffrey', 1288: 'McCall', 1289: 'McCardle', 1290: 'McCarthy', 1291: 'McCartney', 1292: 'McCawley', 1293: 'McClemans', 1294: 'McClinton', 1295: 'McConnell', 1296: 'McCulloch', 1297: 'McDaniels', 1298: 'McDavid', 1299: 'McDonald', 1300: 'McDowell', 1301: 'McElhone', 1302: 'McElroy', 1303: 'McElyea', 1304: 'McEncroe', 1305: 'McEwan', 1306: 'McFarland', 1307: 'McGarry', 1308: 'McGregor', 1309: 'McGuigan', 1310: 'McIntosh', 1311: 'McIntyre', 1312: 'McIver', 1313: 'McKay', 1314: 'McKenzie', 1315: 'McKinley', 1316: 'McKinnon', 1317: 'McKissick', 1318: 'McLean', 1319: 'McMasters', 1320: 'McMillan', 1321: 'McMinn', 1322: 'McMorran', 1323: 'McNaughtan', 1324: 'McNeil', 1325: 'McNeill', 1326: 'McVey', 1327: 'McWilliam', 1328: 'McWilliams', 1329: 'Meany', 1330: 'Medland', 1331: 'Medvedeva', 1332: 'Mello', 1333: 'Mellor', 1334: 'Melton', 1335: 'Meng', 1336: 'Menhennitt', 1337: 'Menkens', 1338: 'Merrett', 1339: 'Messersmith', 1340: 'Metcalfe', 1341: 'Miah', 1342: 'Micco', 1343: 'Michael', 1344: 'Michel', 1345: 'Michelides', 1346: 'Mickey', 1347: 'Micklem', 1348: 'Mikkelsen', 1349: 'Milanesi', 1350: 'Milani', 1351: 'Milano', 1352: 'Miles', 1353: 'Millar', 1354: 'Miller', 1355: 'Milligan', 1356: 'Mills', 1357: 'Milne', 1358: 'Miracle', 1359: 'Mirams', 1360: 'Mironov', 1361: 'Mironova', 1362: 'Mishin', 1363: 'Mishina', 1364: 'Mistry', 1365: 'Mitchel', 1366: 'Mitchell', 1367: 'Molineux', 1368: 'Mollison', 1369: 'Monaldo', 1370: 'Monds', 1371: 'Montalvo', 1372: 'Montemayor', 1373: 'Montes', 1374: 'Montgomery', 1375: 'Moon', 1376: 'Moore', 1377: 'Morales', 1378: 'Moran', 1379: 'Morant', 1380: 'Mordvinova', 1381: 'Morehead', 1382: 'Moretti', 1383: 'Morgan', 1384: 'Morley', 1385: 'Moroney', 1386: 'Morphett', 1387: 'Morres', 1388: 'Morrice', 1389: 'Morris', 1390: 'Morrison', 1391: 'Morton', 1392: 'Moseley', 1393: 'Moss', 1394: 'Mott', 1395: 'Muecke', 1396: 'Muir', 1397: 'Mullah', 1398: 'Mullawirraburka', 1399: 'Munro', 1400: 'Munroe', 1401: 'Muomelu', 1402: 'Muravyova', 1403: 'Murphy', 1404: 'Murray', 1405: 'Muse', 1406: 'Myers', 1407: 'Namatjira', 1408: 'Napolitani', 1409: 'Napolitano', 1410: 'Naquin', 1411: 'Narelle', 1412: 'Naylor', 1413: 'Nazarova', 1414: 'Ndubuagha', 1415: 'Ndubueze', 1416: 'Ndubuisi', 1417: 'Ndukaku', 1418: 'Neal', 1419: 'Nebechi', 1420: 'Nebechukwu', 1421: 'Nebeolisa', 1422: 'Nekrasov', 1423: 'Nekrasova', 1424: 'Nelson', 1425: 'Nepean', 1426: 'Neumann', 1427: 'Neumayer', 1428: 'Nevels', 1429: 'Nevzorova', 1430: 'Newbery', 1431: 'Newland', 1432: 'Newton', 1433: 'Ngozichukwuka', 1434: 'Ni', 1435: 'Nicholas', 1436: 'Nicholls', 1437: 'Nicholson', 1438: 'Nickson', 1439: 'Nicoll', 1440: 'Niehaus', 1441: 'Nielson', 1442: 'Nieves', 1443: 'Nikitina', 1444: 'Niu', 1445: 'Nixon', 1446: 'Nkemakolam', 1447: 'Nkemakonam', 1448: 'Nkemdilim', 1449: 'Nkemdirim', 1450: 'Nkemjika', 1451: 'Nnabuife', 1452: 'Nnachetam', 1453: 'Nnaemeka', 1454: 'Nnaife', 1455: 'Nnamdi', 1456: 'Nnamutaezinwa', 1457: 'Nnanna', 1458: 'Nnonso', 1459: 'Noble', 1460: 'Nock', 1461: 'Norman', 1462: 'Norris', 1463: 'North', 1464: 'Norton', 1465: 'Nott', 1466: 'Nucci', 1467: 'Nwabugwu', 1468: 'Nwachinemelu', 1469: 'Nwachukwu', 1470: 'Nwagugheuzo', 1471: 'Nwankwo', 1472: 'Nwebube', 1473: 'Nweke', 1474: 'Nwokeocha', 1475: 'Nwokezuike', 1476: 'Nwokike', 1477: 'Nwora', 1478: 'Nworie', 1479: 'Nwoye', 1480: 'Nyhan', 1481: \"O'Brien\", 1482: \"O'Callaghan\", 1483: \"O'Connor\", 1484: \"O'Donnell\", 1485: \"O'Kane\", 1486: \"O'Loghlen\", 1487: \"O'Loghlin\", 1488: \"O'Loughlin\", 1489: \"O'Meara\", 1490: \"O'Neill\", 1491: \"O'Sullivan\", 1492: \"O'Toole\", 1493: 'Obiajulu', 1494: 'Obialo', 1495: 'Obidimkpa', 1496: 'Obielumani', 1497: 'Obijiaku', 1498: 'Obinna', 1499: 'Obiora', 1500: 'Obiuto', 1501: 'Ochoa', 1502: 'Odell', 1503: 'Odili', 1504: 'Odinakachukwu', 1505: 'Ofodile', 1506: 'Ogbonnaya', 1507: 'Ogg', 1508: 'Ogochukwu', 1509: 'Oguejiofor', 1510: 'Ojiofor', 1511: 'Okagbue', 1512: 'Okechukwu', 1513: 'Okeke', 1514: 'Okoli', 1515: 'Okonkwo', 1516: 'Okorie', 1517: 'Okwuadigbo', 1518: 'Okwudilichukwu', 1519: 'Okwudiliolisa', 1520: 'Okwukwe', 1521: 'Okwuoma', 1522: 'Olague', 1523: 'Oleary', 1524: 'Olejuru', 1525: 'Olisaemeka', 1526: 'Olisanugo', 1527: 'Oliver', 1528: 'Olsen', 1529: 'Olszewski', 1530: 'Oluchi', 1531: 'Oluchukwu', 1532: 'Omeokachie', 1533: 'Onio', 1534: 'Onochie', 1535: 'Onodugoadiegbemma', 1536: 'Onuchukwu', 1537: 'Onuoha', 1538: 'Onuora', 1539: 'Onwuamaegbu', 1540: 'Onwuamaeze', 1541: 'Onwuatuegwu', 1542: 'Onwubiko', 1543: 'Onwudiwe', 1544: 'Onwuemelie', 1545: 'Onwughara', 1546: 'Onwuka', 1547: 'Onwumelu', 1548: 'Onyekachi', 1549: 'Onyekachukwu', 1550: 'Onyekaozulu', 1551: 'Onyemachukwu', 1552: 'Onyemaechi', 1553: 'Onyemauchechi', 1554: 'Onyemauchechukwu', 1555: 'Onyemere', 1556: 'Onyenachiya', 1557: 'Onyeorulu', 1558: 'Onyeoruru', 1559: 'Onyinyechukwuka', 1560: 'Osborne', 1561: 'Osinachi', 1562: 'Ositadimma', 1563: 'Osonduagwuike', 1564: 'Oster', 1565: 'Otitodilichukwu', 1566: 'Otitodilinna', 1567: 'Otoole', 1568: 'Otutodilichukwu', 1569: 'Otutodilinna', 1570: 'Outhwaite', 1571: 'Owen', 1572: 'Owens', 1573: 'Ozerova', 1574: 'Ozioma', 1575: 'Ozoemena', 1576: 'Ozuluonye', 1577: \"P'an\", 1578: \"P'eng\", 1579: 'Pacheco', 1580: 'Padovano', 1581: 'Padovesi', 1582: 'Pagan', 1583: 'Page', 1584: 'Pagnotto', 1585: 'Pai', 1586: 'Palazzi', 1587: 'Palerma', 1588: 'Palermo', 1589: 'Palfreyman', 1590: 'Paling', 1591: 'Palmer', 1592: 'Pan', 1593: 'Panicucci', 1594: 'Pankhurst', 1595: 'Pape', 1596: 'Pardey', 1597: 'Parker', 1598: 'Parkes', 1599: 'Parkhill', 1600: 'Parkin', 1601: 'Parrott', 1602: 'Parry', 1603: 'Parry-Okeden', 1604: 'Parsons', 1605: 'Patel', 1606: 'Paterson', 1607: 'Patterson', 1608: 'Pauley', 1609: 'Payne', 1610: 'Pearce', 1611: 'Pearson', 1612: 'Peck', 1613: 'Pedder', 1614: 'Pendergrass', 1615: 'Pendred', 1616: 'Peng', 1617: 'Pennington', 1618: 'Peppin', 1619: 'Perez', 1620: 'Perkins', 1621: 'Perrodin', 1622: 'Perry', 1623: 'Pethard', 1624: 'Pettit', 1625: 'Pham', 1626: 'Phelan', 1627: 'Philip', 1628: 'Phillipps', 1629: 'Phillips', 1630: 'Piazza', 1631: 'Piccio', 1632: 'Pickering', 1633: 'Pickworth', 1634: 'Picot', 1635: 'Pino', 1636: 'Pinto', 1637: 'Pipes', 1638: 'Pirogova', 1639: 'Pirozzi', 1640: 'Pisani', 1641: 'Pisano', 1642: 'Pitts', 1643: 'Plumb', 1644: 'Pokrovskaya', 1645: 'Pokrovskii', 1646: 'Pokrovsky', 1647: 'Pollard', 1648: 'Polyakova', 1649: 'Pomeroy', 1650: 'Ponce', 1651: 'Poninski', 1652: 'Ponomarev', 1653: 'Ponomaryov', 1654: 'Ponomaryova', 1655: 'Poole', 1656: 'Pope', 1657: 'Porter', 1658: 'Powell', 1659: 'Power', 1660: 'Pratt', 1661: 'Preston', 1662: 'Price', 1663: 'Pritchard', 1664: 'Prokhorova', 1665: 'Pugh', 1666: 'Pugliesi', 1667: 'Purdy', 1668: 'Pye', 1669: 'Quaife', 1670: 'Quinn', 1671: 'Quinones', 1672: 'Raff', 1673: 'Rahman', 1674: 'Ramos', 1675: 'Ramsbotham', 1676: 'Ramsden', 1677: 'Ramsey', 1678: 'Randall', 1679: 'Rapuluolisa', 1680: 'Rapuokwu', 1681: 'Ratten', 1682: 'Rawlings', 1683: 'Read', 1684: 'Reed', 1685: 'Rees', 1686: 'Reeves', 1687: 'Reid', 1688: 'Reilly', 1689: 'Remington', 1690: 'Rene', 1691: 'Renwick', 1692: 'Repina', 1693: 'Retana', 1694: 'Revell', 1695: 'Reye', 1696: 'Rhodes', 1697: 'Ricci', 1698: 'Rice', 1699: 'Richmond', 1700: 'Rickard', 1701: 'Riddle', 1702: 'Rieke', 1703: 'Riggs', 1704: 'Riley', 1705: 'Rios', 1706: 'Rioux', 1707: 'Rippey', 1708: 'Rischbieth', 1709: 'Rishel', 1710: 'Ritchie', 1711: 'Rivas', 1712: 'Rivers', 1713: 'Rizzo', 1714: 'Robb', 1715: 'Roberts', 1716: 'Robertson', 1717: 'Robinson', 1718: 'Robson', 1719: 'Rocher', 1720: 'Rogers', 1721: 'Rogova', 1722: 'Rohu', 1723: 'Rolon', 1724: 'Romani', 1725: 'Romano', 1726: 'Romilly', 1727: 'Rose', 1728: 'Ross', 1729: 'Rossi', 1730: 'Rounsevell', 1731: 'Rowe', 1732: 'Rowley', 1733: 'Rowntree', 1734: 'Royster', 1735: 'Rozhkova', 1736: 'Rozier', 1737: 'Rubeo', 1738: 'Rubin', 1739: 'Rudduck', 1740: 'Rueda', 1741: 'Ruggiero', 1742: 'Ruiz', 1743: 'Runyon', 1744: 'Ruse', 1745: 'Russell', 1746: 'Russo', 1747: 'Ruth', 1748: 'Rutherford', 1749: 'Ryan', 1750: 'Saad', 1751: 'Sabbatini', 1752: 'Sacco', 1753: 'Sagese', 1754: 'Sal', 1755: 'Salas', 1756: 'Salier', 1757: 'Salmond', 1758: 'Samoylova', 1759: 'Samsonova', 1760: 'Sanchez', 1761: 'Sanders', 1762: 'Sanderson', 1763: 'Sandover', 1764: 'Sani', 1765: 'Santiago', 1766: 'Sargent', 1767: 'Sargood', 1768: 'Saunders', 1769: 'Savage', 1770: 'Sazonova', 1771: 'Schaffer', 1772: 'Schatz', 1773: 'Schiavone', 1774: 'Schmidt', 1775: 'Schneider', 1776: 'Schofield', 1777: 'Scott', 1778: 'Seccombe', 1779: 'Secombe', 1780: 'Seleznev', 1781: 'Selezneva', 1782: 'Seleznyov', 1783: 'Seleznyova', 1784: 'Semmens', 1785: 'Seppelt', 1786: 'Septimus', 1787: 'Serena', 1788: 'Sergeyeva', 1789: 'Service', 1790: 'Severson', 1791: 'Shaffer', 1792: 'Shah', 1793: 'Shand', 1794: 'Shao', 1795: 'Sharp', 1796: 'Sharpe', 1797: 'Shaver', 1798: 'Shaw', 1799: 'Shcherbakov', 1800: 'She', 1801: 'Shearston', 1802: 'Sheehan', 1803: 'Sheets', 1804: 'Shelby', 1805: 'Shelton', 1806: 'Shen', 1807: 'Shephard', 1808: 'Shepherd', 1809: 'Sheppard', 1810: 'Sherman', 1811: 'Sherrod', 1812: 'Shih', 1813: 'Shillito', 1814: 'Shoebridge', 1815: 'Sholes', 1816: 'Shoobridge', 1817: 'Shubin', 1818: 'Shubina', 1819: 'Siciliani', 1820: 'Siciliano', 1821: 'Sidorov', 1822: 'Sievier', 1823: 'Simmons', 1824: 'Simpson', 1825: 'Sims', 1826: 'Sinclair', 1827: 'Singleton', 1828: 'Sinnett', 1829: 'Skelton', 1830: 'Skinner', 1831: 'Slattery', 1832: 'Sleeman', 1833: 'Slone', 1834: 'Small', 1835: 'Smalley', 1836: 'Smeaton', 1837: 'Smith', 1838: 'Sneddon', 1839: 'Soares', 1840: 'Sochima', 1841: 'Sokolov', 1842: 'Sokolova', 1843: 'Solomina', 1844: 'Solomon', 1845: 'Somadina', 1846: 'Somerville', 1847: 'Sopuluchi', 1848: 'Sopuluchukwu', 1849: 'Sorenson', 1850: 'Sorokina', 1851: 'Soto', 1852: 'Sousa', 1853: 'Souter', 1854: 'Sparks', 1855: 'Spaull', 1856: 'Spence', 1857: 'Spencer', 1858: 'Spitzer', 1859: 'Spyer', 1860: 'St Clair', 1861: 'Standish', 1862: 'Stange', 1863: 'Stanley', 1864: 'Stanton', 1865: 'Steele', 1866: 'Steiner', 1867: 'Steinhoff', 1868: 'Stephenson', 1869: 'Stevens', 1870: 'Stevenson', 1871: 'Stewart', 1872: 'Stiles', 1873: 'Stobie', 1874: 'Stone', 1875: 'Stonebraker', 1876: 'Storey', 1877: 'Stout', 1878: 'Stradford', 1879: 'Sturdee', 1880: 'Su', 1881: 'Suffolk', 1882: 'Sugden', 1883: 'Sukhorukova', 1884: 'Sullivan', 1885: 'Summers', 1886: 'Summerville', 1887: 'Sumrall', 1888: 'Sun', 1889: 'Sung', 1890: 'Sutherland', 1891: 'Swaim', 1892: 'Swanson', 1893: 'Swayne', 1894: 'Swift', 1895: 'Synnot', 1896: \"T'an\", 1897: \"T'ang\", 1898: \"T'ao\", 1899: \"T'ien\", 1900: 'Tai', 1901: 'Talbot', 1902: 'Tan', 1903: 'Tang', 1904: 'Tao', 1905: 'Tardent', 1906: 'Taubman', 1907: 'Taylor', 1908: 'Templeman', 1909: 'Teng', 1910: 'Terry', 1911: 'Thao', 1912: 'Thomas', 1913: 'Thompson', 1914: 'Thomson', 1915: 'Thornton', 1916: 'Thorpe', 1917: 'Thurgood', 1918: 'Thynne', 1919: 'Tien', 1920: 'Tikhonov', 1921: 'Tilley', 1922: 'Tillman', 1923: 'Timms', 1924: 'Timperley', 1925: 'Ting', 1926: 'Tinline', 1927: 'Tipton', 1928: 'Tisdall', 1929: 'Titheradge', 1930: 'To Rot', 1931: 'Tobenna', 1932: 'Tobeolisa', 1933: 'Todd', 1934: 'Tokareva', 1935: 'Tokaryev', 1936: 'Tomlinson', 1937: 'Toomey', 1938: 'Topp', 1939: 'Torreggiani', 1940: 'Toscani', 1941: 'Toscano', 1942: 'Toth', 1943: 'Townsend', 1944: 'Townsley', 1945: 'Traeger', 1946: 'Treacy', 1947: 'Trejo', 1948: 'Trentini', 1949: 'Trentino', 1950: 'Tretiakov', 1951: 'Tretiakova', 1952: 'Tretyakova', 1953: 'Trevisan', 1954: 'Trevisani', 1955: 'Trevisano', 1956: 'Trout', 1957: 'Trujillo', 1958: \"Ts'ai\", 1959: \"Ts'ao\", 1960: \"Ts'ui\", 1961: 'Tsai', 1962: 'Tsao', 1963: 'Tseng', 1964: 'Tsou', 1965: 'Tsui', 1966: 'Tu', 1967: 'Tuan', 1968: 'Tucker', 1969: 'Tudawali', 1970: 'Tung', 1971: 'Turnbull', 1972: 'Tychonoff', 1973: 'Tyler', 1974: 'Ubanwa', 1975: 'Uchechukwu', 1976: 'Uchenna', 1977: 'Udegbulam', 1978: 'Udegbunam', 1979: 'Udinese', 1980: 'Udinesi', 1981: 'Udobata', 1982: 'Udokamma', 1983: 'Ugochukwu', 1984: 'Ugochukwutubelum', 1985: 'Ugoji', 1986: 'Ugonna', 1987: 'Ugonnatubelum', 1988: 'Ugorji', 1989: 'Ukaegbulam', 1990: 'Ukaegbunam', 1991: 'Ulyanov', 1992: 'Ulyanova', 1993: 'Unwin', 1994: 'Upchurch', 1995: 'Upjohn', 1996: 'Uren', 1997: 'Uspenskaya', 1998: 'Uspensky', 1999: 'Uvarov', 2000: 'Uvarova', 2001: 'Uwaezuoke', 2002: 'Uwakwe', 2003: 'Vagin', 2004: 'Vaguine', 2005: 'Valdez', 2006: 'Vasiliev', 2007: 'Vasilieva', 2008: 'Vasilyev', 2009: 'Vasilyeva', 2010: 'Vavilov', 2011: 'Velazquez', 2012: 'Verco', 2013: 'Verjus', 2014: 'Vessels', 2015: 'Vicars', 2016: 'Vida', 2017: 'Vidal', 2018: 'Vidler', 2019: 'Vincent', 2020: 'Vinogradoff', 2021: 'Vinogradov', 2022: 'Vinogradova', 2023: 'Virgo', 2024: 'Vogel', 2025: 'Volkova', 2026: 'Von Doussa', 2027: 'Vorobyova', 2028: 'Voronina', 2029: 'Voss', 2030: 'Wade', 2031: 'Wakelin', 2032: 'Walker', 2033: 'Walkom', 2034: 'Wall', 2035: 'Wallace', 2036: 'Wallis', 2037: 'Walsh', 2038: 'Walters', 2039: 'Walton', 2040: 'Wan', 2041: 'Wang', 2042: 'Wanliss', 2043: 'Ward', 2044: 'Waring', 2045: 'Wark', 2046: 'Warlow-Davies', 2047: 'Warner', 2048: 'Waterhouse', 2049: 'Waters', 2050: 'Watkins', 2051: 'Watson', 2052: 'Watt', 2053: 'Watts', 2054: 'Weatherford', 2055: 'Webb', 2056: 'Weber', 2057: 'Wei', 2058: 'Weigel', 2059: 'Welch', 2060: 'Weller', 2061: 'Wentcher', 2062: 'Wentworth-Shields', 2063: 'West', 2064: 'Weston', 2065: 'Wheare', 2066: 'Wheeler', 2067: 'White', 2068: 'Whitehead', 2069: 'Whitehouse', 2070: 'Whitelegge', 2071: 'Whitfield', 2072: 'Whitworth', 2073: 'Wilder', 2074: 'Wildman', 2075: 'Wiley', 2076: 'Wilhelm', 2077: 'Wilkes', 2078: 'Wilkie', 2079: 'Wilkins', 2080: 'Wilkinson', 2081: 'William', 2082: 'Williams', 2083: 'Williamson', 2084: 'Williford', 2085: 'Willis', 2086: 'Willmore', 2087: 'Willoughby', 2088: 'Wilsmore', 2089: 'Wilson', 2090: 'Wimble', 2091: 'Windradyne', 2092: 'Windsor', 2093: 'Winn', 2094: 'Winter-Irving', 2095: 'Winters', 2096: 'Wisdom', 2097: 'Witt', 2098: 'Wollstonecraft', 2099: 'Wong', 2100: 'Wood', 2101: 'Woodhouse', 2102: 'Woods', 2103: 'Woodward', 2104: 'Woolnough', 2105: 'Woronoff', 2106: 'Worsnop', 2107: 'Wreford', 2108: 'Wright', 2109: 'Wu', 2110: 'Wyatt', 2111: 'Wynn', 2112: 'Y?', 2113: 'Y?an', 2114: 'Yamamoto', 2115: 'Yancy', 2116: 'Yang', 2117: 'Yao', 2118: 'Yashina', 2119: 'Yates', 2120: 'Yefimov', 2121: 'Yefimova', 2122: 'Yefremov', 2123: 'Yefremova', 2124: 'Yegorov', 2125: 'Yegorova', 2126: 'Yeh', 2127: 'Yelverton', 2128: 'Yen', 2129: 'Yermakov', 2130: 'Yermakova', 2131: 'Yermolayev', 2132: 'Yermolayeva', 2133: 'Yevdokimova', 2134: 'Yevseyev', 2135: 'Yewen', 2136: 'Yin', 2137: 'Yip', 2138: 'Yirawala', 2139: 'Yobachi', 2140: 'Yobachukwu', 2141: 'Yobanna', 2142: 'Yocum', 2143: 'Yoo', 2144: 'Yost', 2145: 'Young', 2146: 'Younger', 2147: 'Yu', 2148: 'Yuan', 2149: 'Yudin', 2150: 'Yudina', 2151: 'Yuriev', 2152: 'Yuryeva', 2153: 'Yusupova', 2154: 'Zack', 2155: 'Zaitsev', 2156: 'Zakharov', 2157: 'Zaytseva', 2158: 'Zetticci', 2159: 'Zhdanov', 2160: 'Zhdanova', 2161: 'Zhirov', 2162: 'Zhou', 2163: 'Zikoranachidimma', 2164: 'Zikoranachukwudimma', 2165: 'Zikoranaudodimma', 2166: 'Zimmer', 2167: 'Zito', 2168: 'Zox', 2169: 'Zubarev', 2170: 'Zubareva', 2171: 'Zuev', 2172: 'Zuyev', 2173: 'Zuyeva'}\n",
            "Distribusi kolom numerik Surname:\n",
            "Surname\n",
            "909     505\n",
            "1899    429\n",
            "1960    315\n",
            "1013    315\n",
            "1578    312\n",
            "       ... \n",
            "1843      1\n",
            "2013      1\n",
            "244       1\n",
            "1312      1\n",
            "1331      1\n",
            "Name: count, Length: 2174, dtype: int64 \n",
            "\n",
            "Nilai minimum untuk kolom numerik Surname:\n",
            "0 \n",
            "\n",
            "Nilai maksimum untuk kolom numerik Surname:\n",
            "2173 \n",
            "\n",
            "Mapping untuk kolom  non numerikGeography: {0: 'France', 1: 'Germany', 2: 'Spain'}\n",
            "Distribusi kolom numerik Geography:\n",
            "Geography\n",
            "0    18750\n",
            "2     7344\n",
            "1     6940\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Nilai minimum untuk kolom numerik Geography:\n",
            "0 \n",
            "\n",
            "Nilai maksimum untuk kolom numerik Geography:\n",
            "2 \n",
            "\n",
            "Mapping untuk kolom  non numerikGender: {0: 'Female', 1: 'Male'}\n",
            "Distribusi kolom numerik Gender:\n",
            "Gender\n",
            "1    18793\n",
            "0    14241\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Nilai minimum untuk kolom numerik Gender:\n",
            "0 \n",
            "\n",
            "Nilai maksimum untuk kolom numerik Gender:\n",
            "1 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preproses sudah dilakukan dan data siap dipakai, kolom numerik sudah dimapping sehingga sudah menjadi kolom numerik. Tidak ada value data yang didrop atau diubah karena valuenya salah (misalnya ada nilai minus pada kolom yang seharusnya tidak minus)."
      ],
      "metadata": {
        "id": "kd2dF8g00MIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_bank.describe()\n",
        "test_bank.info()\n",
        "train_bank.describe()\n",
        "train_bank.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqFF6-qR6cFK",
        "outputId": "2444779e-d397-4ac9-dbeb-4aef79b6dbfb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 33034 entries, 0 to 33033\n",
            "Data columns (total 14 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   id               33034 non-null  int64  \n",
            " 1   CustomerId       33034 non-null  int64  \n",
            " 2   Surname          33034 non-null  int64  \n",
            " 3   CreditScore      33034 non-null  int64  \n",
            " 4   Geography        33034 non-null  int64  \n",
            " 5   Gender           33034 non-null  int64  \n",
            " 6   Age              33034 non-null  float64\n",
            " 7   Tenure           33034 non-null  int64  \n",
            " 8   Balance          33034 non-null  float64\n",
            " 9   NumOfProducts    33034 non-null  int64  \n",
            " 10  HasCrCard        33034 non-null  float64\n",
            " 11  IsActiveMember   33034 non-null  float64\n",
            " 12  EstimatedSalary  33034 non-null  float64\n",
            " 13  Exited           33034 non-null  int64  \n",
            "dtypes: float64(5), int64(9)\n",
            "memory usage: 3.5 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 132000 entries, 0 to 131999\n",
            "Data columns (total 14 columns):\n",
            " #   Column           Non-Null Count   Dtype  \n",
            "---  ------           --------------   -----  \n",
            " 0   id               132000 non-null  int64  \n",
            " 1   CustomerId       132000 non-null  int64  \n",
            " 2   Surname          132000 non-null  int64  \n",
            " 3   CreditScore      132000 non-null  int64  \n",
            " 4   Geography        132000 non-null  int64  \n",
            " 5   Gender           132000 non-null  int64  \n",
            " 6   Age              132000 non-null  float64\n",
            " 7   Tenure           132000 non-null  int64  \n",
            " 8   Balance          132000 non-null  float64\n",
            " 9   NumOfProducts    132000 non-null  int64  \n",
            " 10  HasCrCard        132000 non-null  float64\n",
            " 11  IsActiveMember   132000 non-null  float64\n",
            " 12  EstimatedSalary  132000 non-null  float64\n",
            " 13  Exited           132000 non-null  int64  \n",
            "dtypes: float64(5), int64(9)\n",
            "memory usage: 14.1 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tahap 2 : Pembuatan Model Klasifikasi Dengan Algoritma Decision Tree"
      ],
      "metadata": {
        "id": "TqwofzXJ6gdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eksperimen pemilihan fitur (kolom) terbaik"
      ],
      "metadata": {
        "id": "M5ppmNSaJ0ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_bank.drop(columns=['Exited', 'id', 'CustomerId'])\n",
        "y = train_bank['Exited']\n",
        "\n",
        "x_test = test_bank.drop(columns=['Exited', 'id', 'CustomerId'])\n",
        "y_test = test_bank['Exited']\n",
        "\n",
        "accuracy_scores_by_k ={}\n",
        "\n",
        "for k in range(1, 12):\n",
        "    # Pilih k fitur terbaik berdasarkan Chi-Square\n",
        "    selector = SelectKBest(chi2, k=k)\n",
        "    x_selected = selector.fit_transform(x, y)\n",
        "    top_columns = x.columns[selector.get_support()]\n",
        "    print(f\"Jumlah fitur terpilih (k={k}):\", top_columns.tolist())\n",
        "    # Terapkan fitur yang dipilih ke dataset uji\n",
        "    x_test_selected = selector.transform(x_test)\n",
        "\n",
        "    # Buat model Decision Tree\n",
        "    dt_model = DecisionTreeClassifier(random_state=42)\n",
        "    dt_model.fit(x_selected, y)\n",
        "\n",
        "    # Prediksi data uji\n",
        "    y_pred = dt_model.predict(x_test_selected)\n",
        "\n",
        "    # Evaluasi model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    accuracy_scores_by_k[(k)] = (top_columns,accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb-mEKiQ7YIL",
        "outputId": "30f06aa3-f8da-4184-dd30-dfae4fe176a5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah fitur terpilih (k=1): ['Balance']\n",
            "Accuracy: 0.7640915420475873\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.95      0.86     26019\n",
            "           1       0.31      0.09      0.14      7015\n",
            "\n",
            "    accuracy                           0.76     33034\n",
            "   macro avg       0.55      0.52      0.50     33034\n",
            "weighted avg       0.69      0.76      0.71     33034\n",
            "\n",
            "Jumlah fitur terpilih (k=2): ['Balance', 'EstimatedSalary']\n",
            "Accuracy: 0.722316401283526\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.87      0.83     26019\n",
            "           1       0.27      0.19      0.22      7015\n",
            "\n",
            "    accuracy                           0.72     33034\n",
            "   macro avg       0.54      0.53      0.53     33034\n",
            "weighted avg       0.69      0.72      0.70     33034\n",
            "\n",
            "Jumlah fitur terpilih (k=3): ['Surname', 'Balance', 'EstimatedSalary']\n",
            "Accuracy: 0.6657383302052431\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.78      0.79     26019\n",
            "           1       0.23      0.24      0.23      7015\n",
            "\n",
            "    accuracy                           0.67     33034\n",
            "   macro avg       0.51      0.51      0.51     33034\n",
            "weighted avg       0.67      0.67      0.67     33034\n",
            "\n",
            "Jumlah fitur terpilih (k=4): ['Surname', 'Age', 'Balance', 'EstimatedSalary']\n",
            "Accuracy: 0.7250105951443967\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.82      0.82     26019\n",
            "           1       0.36      0.37      0.36      7015\n",
            "\n",
            "    accuracy                           0.73     33034\n",
            "   macro avg       0.59      0.59      0.59     33034\n",
            "weighted avg       0.73      0.73      0.73     33034\n",
            "\n",
            "Jumlah fitur terpilih (k=5): ['Surname', 'Age', 'Balance', 'IsActiveMember', 'EstimatedSalary']\n",
            "Accuracy: 0.73866319549555\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.83      0.83     26019\n",
            "           1       0.39      0.41      0.40      7015\n",
            "\n",
            "    accuracy                           0.74     33034\n",
            "   macro avg       0.61      0.62      0.62     33034\n",
            "weighted avg       0.74      0.74      0.74     33034\n",
            "\n",
            "Jumlah fitur terpilih (k=6): ['Surname', 'Gender', 'Age', 'Balance', 'IsActiveMember', 'EstimatedSalary']\n",
            "Accuracy: 0.7484410001816311\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84     26019\n",
            "           1       0.41      0.42      0.42      7015\n",
            "\n",
            "    accuracy                           0.75     33034\n",
            "   macro avg       0.63      0.63      0.63     33034\n",
            "weighted avg       0.75      0.75      0.75     33034\n",
            "\n",
            "Jumlah fitur terpilih (k=7): ['Surname', 'Gender', 'Age', 'Balance', 'NumOfProducts', 'IsActiveMember', 'EstimatedSalary']\n",
            "Accuracy: 0.7934552279469638\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87     26019\n",
            "           1       0.51      0.52      0.52      7015\n",
            "\n",
            "    accuracy                           0.79     33034\n",
            "   macro avg       0.69      0.69      0.69     33034\n",
            "weighted avg       0.79      0.79      0.79     33034\n",
            "\n",
            "Jumlah fitur terpilih (k=8): ['Surname', 'CreditScore', 'Gender', 'Age', 'Balance', 'NumOfProducts', 'IsActiveMember', 'EstimatedSalary']\n",
            "Accuracy: 0.7894290730762245\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.87     26019\n",
            "           1       0.50      0.52      0.51      7015\n",
            "\n",
            "    accuracy                           0.79     33034\n",
            "   macro avg       0.69      0.69      0.69     33034\n",
            "weighted avg       0.79      0.79      0.79     33034\n",
            "\n",
            "Jumlah fitur terpilih (k=9): ['Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Balance', 'NumOfProducts', 'IsActiveMember', 'EstimatedSalary']\n",
            "Accuracy: 0.7973300236120361\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87     26019\n",
            "           1       0.52      0.54      0.53      7015\n",
            "\n",
            "    accuracy                           0.80     33034\n",
            "   macro avg       0.70      0.70      0.70     33034\n",
            "weighted avg       0.80      0.80      0.80     33034\n",
            "\n",
            "Jumlah fitur terpilih (k=10): ['Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'IsActiveMember', 'EstimatedSalary']\n",
            "Accuracy: 0.7985408972573712\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.87     26019\n",
            "           1       0.52      0.54      0.53      7015\n",
            "\n",
            "    accuracy                           0.80     33034\n",
            "   macro avg       0.70      0.70      0.70     33034\n",
            "weighted avg       0.80      0.80      0.80     33034\n",
            "\n",
            "Jumlah fitur terpilih (k=11): ['Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
            "Accuracy: 0.7956045286674336\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.87     26019\n",
            "           1       0.52      0.54      0.53      7015\n",
            "\n",
            "    accuracy                           0.80     33034\n",
            "   macro avg       0.70      0.70      0.70     33034\n",
            "weighted avg       0.80      0.80      0.80     33034\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print (accuracy_scores_by_k)\n",
        "res = sorted(accuracy_scores_by_k.items(), key=lambda x: x[1][1], reverse=True)\n",
        "\n",
        "best_k , (best_col, best_accuracy) = res[0]\n",
        "\n",
        "print ('Model terbaik berdasarkan accuracy :')\n",
        "print (f'Jumlah Atribut Terbaik = {best_k}')\n",
        "print (f'Dengan Kolom Atribut Terbaik = {best_col}')\n",
        "print (f'Dengan Accuracy Terbaik = {best_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC1y6NOME1OF",
        "outputId": "5e86475e-ae4d-42ea-9d49-daab3a514f67"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model terbaik berdasarkan accuracy :\n",
            "Jumlah Atribut Terbaik = 10\n",
            "Dengan Kolom Atribut Terbaik = Index(['Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure',\n",
            "       'Balance', 'NumOfProducts', 'IsActiveMember', 'EstimatedSalary'],\n",
            "      dtype='object')\n",
            "Dengan Accuracy Terbaik = 0.7985408972573712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kesimpulan :\n",
        "\n",
        "Model terbaik berdasarkan akurasi\n",
        "\n",
        "Jumlah Atribut Terbaik : 10\n",
        "\n",
        "Dengan Kolom Atribut Terbaik  (yang diurutkan berdasarkan chi2):\n",
        "1. 'Surname'\n",
        "2. 'CreditScore'\n",
        "3. 'Geography'\n",
        "4. 'Gender'\n",
        "5. 'Age'\n",
        "6. 'Tenure'\n",
        "7. 'Balance'\n",
        "8. 'NumOfProducts'\n",
        "9. 'IsActiveMember'\n",
        "10. 'EstimatedSalary'\n",
        "\n",
        "Dengan Accuracy Terbaik = 0.7985408972573712"
      ],
      "metadata": {
        "id": "g2S6q7HhIL15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pembuatan Model dengan Fitur terbaik"
      ],
      "metadata": {
        "id": "aatdx0q2KBNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_columns = ['Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'IsActiveMember', 'EstimatedSalary']\n",
        "x_selected= train_bank[best_columns]\n",
        "y = train_bank['Exited']\n",
        "\n",
        "x_test_selected = test_bank[best_columns]\n",
        "y_test = test_bank['Exited']\n",
        "\n",
        "\n",
        "# Buat model Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(x_selected, y)\n",
        "# Prediksi data uji\n",
        "y_pred = dt_model.predict(x_test_selected)\n",
        "# Evaluasi model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "parA4cIIKIYC",
        "outputId": "85299a1d-56e2-4f91-c25a-3089cd551777"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7985408972573712\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.87     26019\n",
            "           1       0.52      0.54      0.53      7015\n",
            "\n",
            "    accuracy                           0.80     33034\n",
            "   macro avg       0.70      0.70      0.70     33034\n",
            "weighted avg       0.80      0.80      0.80     33034\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kesimpulan :\n",
        "- Akurasi global = 0.7985408972573712\n",
        "\n",
        "- **Kelas terbaik berdasarkan F1-Score**: Kelas 0 (No Exit) dengan f1-score: 0.87\n",
        "- **Kelas terburuk berdasarkan F1-Score**: Kelas 1 (Exited) dengan f1-score: 0.53"
      ],
      "metadata": {
        "id": "Csdp91rfLoxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tahap 3 : Random Forest\n"
      ],
      "metadata": {
        "id": "ab6rgtCrM1qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan model dt tersebut, dapatkan terlebih dahulu hyper paramsnya, berikut ini adalah hasilnya:\n",
        "1. max depth = none, yang berarti pohon tidak dibatasi dalamnya. pohon akan tetap terus bertambah dalamnya sampai didapatkan seluruh node leafnya terdiri dari hanya satu kelas.\n",
        "2. min sample split = 2, yang artinya pohon akan tetap mencoba split sebuha node jika terdapat minimal 2 sample, sehingga pohon akan tetap bertambah panjang dalamnya hingga tedapat kurang dari 2 sample pada leafnya. hal ini dapat membuat model lebih terhidanr dari under fit\n",
        "3. min sample leaf = 1, yang artinya harus ada minmal 1 sample pada setiap node. hal ini akan membuat pohon akan terus bertmabh panjang dalamnya dan mencegah under fit\n"
      ],
      "metadata": {
        "id": "dkJi3apBWLdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_max_depth = dt_model.get_params()['max_depth']\n",
        "dt_min_samples_split = dt_model.get_params()['min_samples_split']\n",
        "dt_min_samples_leaf = dt_model.get_params()['min_samples_leaf']\n"
      ],
      "metadata": {
        "id": "EpKKVBHyVOoN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "max_depths = [dt_max_depth, 5, 10]\n",
        "min_samples_splits = [dt_min_samples_split, 10, 20]\n",
        "min_samples_leafs = [dt_min_samples_leaf, 10, 15]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "for max_depth in max_depths:\n",
        "  for min_samples_split in min_samples_splits:\n",
        "    for min_samples_leaf in min_samples_leafs:\n",
        "      # rf model use dt hyperparam\n",
        "      rf_model = RandomForestClassifier(\n",
        "          n_estimators=100,  # Number of trees in the Random Forest\n",
        "          max_depth = max_depth,\n",
        "          min_samples_split = min_samples_split,\n",
        "          min_samples_leaf = min_samples_leaf,\n",
        "          random_state=42\n",
        "      )\n",
        "\n",
        "      rf_model.fit(x_selected, y)\n",
        "\n",
        "      y_pred_rf = rf_model.predict(x_test_selected)\n",
        "\n",
        "      accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "      report_rf = classification_report(y_test, y_pred_rf)\n",
        "      conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "      if  accuracy_rf > best_accuracy:\n",
        "        best_accuracy = accuracy_rf\n",
        "        best_params = (max_depth, min_samples_split, min_samples_leaf)\n",
        "      # Print results\n",
        "      print (\"-----------------\")\n",
        "      print (f\"max_depth = {max_depth}, min_samples_split = {min_samples_split}, min_samples_leaf = {min_samples_leaf}\")\n",
        "      # Print evaluation metrics\n",
        "      print(\"Accuracy (Random Forest):\", accuracy_rf)\n",
        "      print(\"\\nClassification Report (Random Forest):\")\n",
        "      print(report_rf)\n",
        "      print(\"\\nConfusion Matrix (Random Forest):\")\n",
        "      print(conf_matrix_rf)\n"
      ],
      "metadata": {
        "id": "egQFAPOOM4Ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8cba870-39c2-4a5a-bcda-8a161c86e497"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "max_depth = None, min_samples_split = 2, min_samples_leaf = 1\n",
            "Accuracy (Random Forest): 0.8589332203184598\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.91     26019\n",
            "           1       0.73      0.53      0.62      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.81      0.74      0.76     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24637  1382]\n",
            " [ 3278  3737]]\n",
            "-----------------\n",
            "max_depth = None, min_samples_split = 2, min_samples_leaf = 10\n",
            "Accuracy (Random Forest): 0.8627474723012654\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92     26019\n",
            "           1       0.75      0.53      0.62      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.74      0.77     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24760  1259]\n",
            " [ 3275  3740]]\n",
            "-----------------\n",
            "max_depth = None, min_samples_split = 2, min_samples_leaf = 15\n",
            "Accuracy (Random Forest): 0.8623842102076649\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92     26019\n",
            "           1       0.75      0.53      0.62      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.74      0.77     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24779  1240]\n",
            " [ 3306  3709]]\n",
            "-----------------\n",
            "max_depth = None, min_samples_split = 10, min_samples_leaf = 1\n",
            "Accuracy (Random Forest): 0.8615365986559302\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92     26019\n",
            "           1       0.74      0.53      0.62      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.81      0.74      0.77     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24708  1311]\n",
            " [ 3263  3752]]\n",
            "-----------------\n",
            "max_depth = None, min_samples_split = 10, min_samples_leaf = 10\n",
            "Accuracy (Random Forest): 0.8627474723012654\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92     26019\n",
            "           1       0.75      0.53      0.62      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.74      0.77     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24760  1259]\n",
            " [ 3275  3740]]\n",
            "-----------------\n",
            "max_depth = None, min_samples_split = 10, min_samples_leaf = 15\n",
            "Accuracy (Random Forest): 0.8623842102076649\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92     26019\n",
            "           1       0.75      0.53      0.62      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.74      0.77     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24779  1240]\n",
            " [ 3306  3709]]\n",
            "-----------------\n",
            "max_depth = None, min_samples_split = 20, min_samples_leaf = 1\n",
            "Accuracy (Random Forest): 0.8626566567778652\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92     26019\n",
            "           1       0.75      0.54      0.62      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.81      0.74      0.77     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24730  1289]\n",
            " [ 3248  3767]]\n",
            "-----------------\n",
            "max_depth = None, min_samples_split = 20, min_samples_leaf = 10\n",
            "Accuracy (Random Forest): 0.8627474723012654\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92     26019\n",
            "           1       0.75      0.53      0.62      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.74      0.77     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24760  1259]\n",
            " [ 3275  3740]]\n",
            "-----------------\n",
            "max_depth = None, min_samples_split = 20, min_samples_leaf = 15\n",
            "Accuracy (Random Forest): 0.8623842102076649\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92     26019\n",
            "           1       0.75      0.53      0.62      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.74      0.77     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24779  1240]\n",
            " [ 3306  3709]]\n",
            "-----------------\n",
            "max_depth = 5, min_samples_split = 2, min_samples_leaf = 1\n",
            "Accuracy (Random Forest): 0.8515468910819156\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91     26019\n",
            "           1       0.79      0.41      0.54      7015\n",
            "\n",
            "    accuracy                           0.85     33034\n",
            "   macro avg       0.83      0.69      0.72     33034\n",
            "weighted avg       0.84      0.85      0.83     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[25270   749]\n",
            " [ 4155  2860]]\n",
            "-----------------\n",
            "max_depth = 5, min_samples_split = 2, min_samples_leaf = 10\n",
            "Accuracy (Random Forest): 0.8514560755585154\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91     26019\n",
            "           1       0.79      0.41      0.54      7015\n",
            "\n",
            "    accuracy                           0.85     33034\n",
            "   macro avg       0.83      0.69      0.72     33034\n",
            "weighted avg       0.84      0.85      0.83     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[25282   737]\n",
            " [ 4170  2845]]\n",
            "-----------------\n",
            "max_depth = 5, min_samples_split = 2, min_samples_leaf = 15\n",
            "Accuracy (Random Forest): 0.8506084640067809\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91     26019\n",
            "           1       0.79      0.40      0.53      7015\n",
            "\n",
            "    accuracy                           0.85     33034\n",
            "   macro avg       0.82      0.69      0.72     33034\n",
            "weighted avg       0.84      0.85      0.83     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[25273   746]\n",
            " [ 4189  2826]]\n",
            "-----------------\n",
            "max_depth = 5, min_samples_split = 10, min_samples_leaf = 1\n",
            "Accuracy (Random Forest): 0.850699279530181\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91     26019\n",
            "           1       0.79      0.40      0.53      7015\n",
            "\n",
            "    accuracy                           0.85     33034\n",
            "   macro avg       0.82      0.69      0.72     33034\n",
            "weighted avg       0.84      0.85      0.83     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[25273   746]\n",
            " [ 4186  2829]]\n",
            "-----------------\n",
            "max_depth = 5, min_samples_split = 10, min_samples_leaf = 10\n",
            "Accuracy (Random Forest): 0.8514560755585154\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91     26019\n",
            "           1       0.79      0.41      0.54      7015\n",
            "\n",
            "    accuracy                           0.85     33034\n",
            "   macro avg       0.83      0.69      0.72     33034\n",
            "weighted avg       0.84      0.85      0.83     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[25282   737]\n",
            " [ 4170  2845]]\n",
            "-----------------\n",
            "max_depth = 5, min_samples_split = 10, min_samples_leaf = 15\n",
            "Accuracy (Random Forest): 0.8506084640067809\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91     26019\n",
            "           1       0.79      0.40      0.53      7015\n",
            "\n",
            "    accuracy                           0.85     33034\n",
            "   macro avg       0.82      0.69      0.72     33034\n",
            "weighted avg       0.84      0.85      0.83     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[25273   746]\n",
            " [ 4189  2826]]\n",
            "-----------------\n",
            "max_depth = 5, min_samples_split = 20, min_samples_leaf = 1\n",
            "Accuracy (Random Forest): 0.8514258037173821\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91     26019\n",
            "           1       0.79      0.41      0.54      7015\n",
            "\n",
            "    accuracy                           0.85     33034\n",
            "   macro avg       0.83      0.69      0.72     33034\n",
            "weighted avg       0.84      0.85      0.83     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[25269   750]\n",
            " [ 4158  2857]]\n",
            "-----------------\n",
            "max_depth = 5, min_samples_split = 20, min_samples_leaf = 10\n",
            "Accuracy (Random Forest): 0.8514560755585154\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91     26019\n",
            "           1       0.79      0.41      0.54      7015\n",
            "\n",
            "    accuracy                           0.85     33034\n",
            "   macro avg       0.83      0.69      0.72     33034\n",
            "weighted avg       0.84      0.85      0.83     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[25282   737]\n",
            " [ 4170  2845]]\n",
            "-----------------\n",
            "max_depth = 5, min_samples_split = 20, min_samples_leaf = 15\n",
            "Accuracy (Random Forest): 0.8506084640067809\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.97      0.91     26019\n",
            "           1       0.79      0.40      0.53      7015\n",
            "\n",
            "    accuracy                           0.85     33034\n",
            "   macro avg       0.82      0.69      0.72     33034\n",
            "weighted avg       0.84      0.85      0.83     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[25273   746]\n",
            " [ 4189  2826]]\n",
            "-----------------\n",
            "max_depth = 10, min_samples_split = 2, min_samples_leaf = 1\n",
            "Accuracy (Random Forest): 0.862959375189199\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92     26019\n",
            "           1       0.76      0.51      0.61      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.74      0.77     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24910  1109]\n",
            " [ 3418  3597]]\n",
            "-----------------\n",
            "max_depth = 10, min_samples_split = 2, min_samples_leaf = 10\n",
            "Accuracy (Random Forest): 0.8622631228431313\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92     26019\n",
            "           1       0.76      0.51      0.61      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.73      0.76     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24906  1113]\n",
            " [ 3437  3578]]\n",
            "-----------------\n",
            "max_depth = 10, min_samples_split = 2, min_samples_leaf = 15\n",
            "Accuracy (Random Forest): 0.8626263849367318\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92     26019\n",
            "           1       0.77      0.51      0.61      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.73      0.76     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24926  1093]\n",
            " [ 3445  3570]]\n",
            "-----------------\n",
            "max_depth = 10, min_samples_split = 10, min_samples_leaf = 1\n",
            "Accuracy (Random Forest): 0.8628382878246655\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92     26019\n",
            "           1       0.77      0.51      0.61      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.73      0.76     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24934  1085]\n",
            " [ 3446  3569]]\n",
            "-----------------\n",
            "max_depth = 10, min_samples_split = 10, min_samples_leaf = 10\n",
            "Accuracy (Random Forest): 0.8622631228431313\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92     26019\n",
            "           1       0.76      0.51      0.61      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.73      0.76     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24906  1113]\n",
            " [ 3437  3578]]\n",
            "-----------------\n",
            "max_depth = 10, min_samples_split = 10, min_samples_leaf = 15\n",
            "Accuracy (Random Forest): 0.8626263849367318\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92     26019\n",
            "           1       0.77      0.51      0.61      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.73      0.76     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24926  1093]\n",
            " [ 3445  3570]]\n",
            "-----------------\n",
            "max_depth = 10, min_samples_split = 20, min_samples_leaf = 1\n",
            "Accuracy (Random Forest): 0.8628988315069323\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92     26019\n",
            "           1       0.77      0.51      0.61      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.73      0.76     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24940  1079]\n",
            " [ 3450  3565]]\n",
            "-----------------\n",
            "max_depth = 10, min_samples_split = 20, min_samples_leaf = 10\n",
            "Accuracy (Random Forest): 0.8622631228431313\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92     26019\n",
            "           1       0.76      0.51      0.61      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.73      0.76     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24906  1113]\n",
            " [ 3437  3578]]\n",
            "-----------------\n",
            "max_depth = 10, min_samples_split = 20, min_samples_leaf = 15\n",
            "Accuracy (Random Forest): 0.8626263849367318\n",
            "\n",
            "Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92     26019\n",
            "           1       0.77      0.51      0.61      7015\n",
            "\n",
            "    accuracy                           0.86     33034\n",
            "   macro avg       0.82      0.73      0.76     33034\n",
            "weighted avg       0.85      0.86      0.85     33034\n",
            "\n",
            "\n",
            "Confusion Matrix (Random Forest):\n",
            "[[24926  1093]\n",
            " [ 3445  3570]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (best_accuracy)\n",
        "print (best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K2lvYuDjTLM",
        "outputId": "3ecf51fd-3e05-4b93-c389-5cd80dc5efa7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.862959375189199\n",
            "(10, 2, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kesimpulan : setelah dilakukan eskplorasi didapatkan sebuah model RF terbaik berdasarkan akurasinya yaitu ** 86,29%** , dengan param max_depth = 10, min_samples_split = 2, min_samples_leaf = 1. Dibandingkan dengan akurasi dt yaitu **79.85%**\n",
        "\n"
      ],
      "metadata": {
        "id": "cLvBtrWijZz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Pemanfaatan Model"
      ],
      "metadata": {
        "id": "riPZ1Xko5lOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model sesuai hyper param terbaik yang sudah didapatkan sebelumnya"
      ],
      "metadata": {
        "id": "VFXT6ezL7EPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model = RandomForestClassifier(\n",
        "          n_estimators=100,  # Number of trees in the Random Forest\n",
        "          max_depth = 10,\n",
        "          min_samples_split = 2,\n",
        "          min_samples_leaf = 1,\n",
        "          random_state=42)\n",
        "\n",
        "rf_model.fit(x_selected, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "3Vz9v0i2jZf6",
        "outputId": "9e1c0c65-2a40-4ab8-9051-ea7c7f5a5a12"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(max_depth=10, random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=10, random_state=42)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data baru\n"
      ],
      "metadata": {
        "id": "mBFK260J7JNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    {\"id\": 91, \"CustomerId\": 15579526, \"Surname\": \"Niu\", \"CreditScore\": 743, \"Geography\": \"Germany\", \"Gender\": \"Male\", \"Age\": 37.0, \"Tenure\": 2, \"Balance\": 132627.51, \"NumOfProducts\": 1, \"HasCrCard\": 1.0, \"IsActiveMember\": 0.0, \"EstimatedSalary\": 183566.87, \"Exited\": 0},\n",
        "    {\"id\": 92, \"CustomerId\": 15623082, \"Surname\": \"Ndukaku\", \"CreditScore\": 726, \"Geography\": \"France\", \"Gender\": \"Female\", \"Age\": 26.0, \"Tenure\": 5, \"Balance\": 0.0, \"NumOfProducts\": 2, \"HasCrCard\": 1.0, \"IsActiveMember\": 0.0, \"EstimatedSalary\": 52449.97, \"Exited\": 0},\n",
        "    {\"id\": 93, \"CustomerId\": 15641822, \"Surname\": \"Mironova\", \"CreditScore\": 431, \"Geography\": \"France\", \"Gender\": \"Male\", \"Age\": 37.0, \"Tenure\": 4, \"Balance\": 0.0, \"NumOfProducts\": 2, \"HasCrCard\": 1.0, \"IsActiveMember\": 1.0, \"EstimatedSalary\": 171344.06, \"Exited\": 0},\n",
        "    {\"id\": 94, \"CustomerId\": 15756875, \"Surname\": \"Johnston\", \"CreditScore\": 571, \"Geography\": \"France\", \"Gender\": \"Female\", \"Age\": 50.0, \"Tenure\": 4, \"Balance\": 0.0, \"NumOfProducts\": 1, \"HasCrCard\": 1.0, \"IsActiveMember\": 0.0, \"EstimatedSalary\": 145567.36, \"Exited\": 0},\n",
        "    {\"id\": 95, \"CustomerId\": 15791534, \"Surname\": \"Scott\", \"CreditScore\": 588, \"Geography\": \"Germany\", \"Gender\": \"Male\", \"Age\": 30.0, \"Tenure\": 10, \"Balance\": 126683.4, \"NumOfProducts\": 1, \"HasCrCard\": 1.0, \"IsActiveMember\": 1.0, \"EstimatedSalary\": 131636.55, \"Exited\": 0},\n",
        "    {\"id\": 96, \"CustomerId\": 15671139, \"Surname\": \"Shih\", \"CreditScore\": 659, \"Geography\": \"Spain\", \"Gender\": \"Female\", \"Age\": 39.0, \"Tenure\": 0, \"Balance\": 107042.74, \"NumOfProducts\": 1, \"HasCrCard\": 1.0, \"IsActiveMember\": 0.0, \"EstimatedSalary\": 102284.2, \"Exited\": 0},\n",
        "]\n",
        "\n",
        "# value EXITEDnya 0, tetapi tidak berarti tergabung ke kelas 0, hanya default inisialisasi\n",
        "\n",
        "data = pd.DataFrame(data)\n",
        "preprocess(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unYGkudX7LZ9",
        "outputId": "6f032b10-68a2-4433-b48a-d3d6fc1ee3f6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping untuk kolom  non numerikSurname: {0: 'Johnston', 1: 'Mironova', 2: 'Ndukaku', 3: 'Niu', 4: 'Scott', 5: 'Shih'}\n",
            "Distribusi kolom numerik Surname:\n",
            "Surname\n",
            "3    1\n",
            "2    1\n",
            "1    1\n",
            "0    1\n",
            "4    1\n",
            "5    1\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Nilai minimum untuk kolom numerik Surname:\n",
            "0 \n",
            "\n",
            "Nilai maksimum untuk kolom numerik Surname:\n",
            "5 \n",
            "\n",
            "Mapping untuk kolom  non numerikGeography: {0: 'France', 1: 'Germany', 2: 'Spain'}\n",
            "Distribusi kolom numerik Geography:\n",
            "Geography\n",
            "0    3\n",
            "1    2\n",
            "2    1\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Nilai minimum untuk kolom numerik Geography:\n",
            "0 \n",
            "\n",
            "Nilai maksimum untuk kolom numerik Geography:\n",
            "2 \n",
            "\n",
            "Mapping untuk kolom  non numerikGender: {0: 'Female', 1: 'Male'}\n",
            "Distribusi kolom numerik Gender:\n",
            "Gender\n",
            "1    3\n",
            "0    3\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Nilai minimum untuk kolom numerik Gender:\n",
            "0 \n",
            "\n",
            "Nilai maksimum untuk kolom numerik Gender:\n",
            "1 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Hasil Prediksi"
      ],
      "metadata": {
        "id": "6rd4AJhS7bTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_columns = ['Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'IsActiveMember', 'EstimatedSalary']\n",
        "data_selected = data[best_columns]\n",
        "\n",
        "y = rf_model.predict(data_selected)\n",
        "data['Exited'] = y\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JimC_UcU7c0C",
        "outputId": "a1fa4b02-e565-42a0-add8-7432e55d587f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  CustomerId  Surname  CreditScore  Geography  Gender   Age  Tenure  \\\n",
            "0  91    15579526        3          743          1       1  37.0       2   \n",
            "1  92    15623082        2          726          0       0  26.0       5   \n",
            "2  93    15641822        1          431          0       1  37.0       4   \n",
            "3  94    15756875        0          571          0       0  50.0       4   \n",
            "4  95    15791534        4          588          1       1  30.0      10   \n",
            "5  96    15671139        5          659          2       0  39.0       0   \n",
            "\n",
            "     Balance  NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \\\n",
            "0  132627.51              1        1.0             0.0        183566.87   \n",
            "1       0.00              2        1.0             0.0         52449.97   \n",
            "2       0.00              2        1.0             1.0        171344.06   \n",
            "3       0.00              1        1.0             0.0        145567.36   \n",
            "4  126683.40              1        1.0             1.0        131636.55   \n",
            "5  107042.74              1        1.0             0.0        102284.20   \n",
            "\n",
            "   Exited  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       1  \n",
            "4       0  \n",
            "5       0  \n"
          ]
        }
      ]
    }
  ]
}